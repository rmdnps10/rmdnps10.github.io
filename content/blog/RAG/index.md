---
title: "RAGì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ìŠµ"
date: "2025-10-01"
description: "RAGì˜ í•µì‹¬ ê°œë…ë¶€í„° ë²¡í„° DB êµ¬ì¶•, ì‹¤ì œ ì±—ë´‡ êµ¬í˜„ê¹Œì§€ Python ì½”ë“œì™€ í•¨ê»˜ ì•Œì•„ë³´ì."
thumbnail: "./index.png"
pointColor: "#FF6B6B"
tags: ["AI"]
---

> ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì‹¤ì‹œê°„ ì •ë³´ í™œìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” RAG ê¸°ìˆ ì´ ì£¼ëª©ë°›ê³  ìˆë‹¤. ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì´ í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì„ ì´ë¡ ë¶€í„° ì‹¤ìŠµê¹Œì§€ ì•Œì•„ë³´ì.

## 1ï¸âƒ£ RAGë€?

#### 1-1 RAGì˜ ì •ì˜

`RAG(Retrieval-Augmented Generation)`ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ë¡œ **ì •ë³´ ê²€ìƒ‰ì„ í†µí•´** ì–¸ì–´ ìƒì„± ê³¼ì •ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ì‹ì´ë‹¤.

ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ì´ ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë°˜ë©´, RAGëŠ” ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” ì •í™•í•˜ê³  ìµœì‹ ì˜ ë‹µë³€ì„ ìƒì„±í•œë‹¤.

#### 1-2 RAG ì‘ë™ ì›ë¦¬

RAGì˜ ì‘ë™ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

1. **ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•œë‹¤**
2. **RAGëŠ” ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•œë‹¤**
3. **ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•œë‹¤**

```python
# RAG ì‘ë™ ì›ë¦¬ ì˜ˆì‹œ
def rag_process(user_query):
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    query = user_query

    # 2. ì™¸ë¶€ DBì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    relevant_docs = vector_db.search(query, top_k=3)

    # 3. ê²€ìƒ‰ëœ ì •ë³´ì™€ í•¨ê»˜ LLMì— ì „ë‹¬
    context = "\n".join(relevant_docs)
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"

    # 4. ìµœì¢… ë‹µë³€ ìƒì„±
    answer = llm.generate(prompt)
    return answer
```

## 2ï¸âƒ£ RAGëŠ” ì™œ í•„ìš”í• ê¹Œ?

#### 2-1 ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„

ChatGPTì™€ ê°™ì€ **ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸**ë„ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ë¥¼ ê°€ì§„ë‹¤:

- **Knowledge Cutoff**: í•™ìŠµ ì´í›„ì— ìƒˆë¡œ ë‚˜ì˜¨ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ì´ ì–´ë ¤ì›€
- **Training Data Cutoff**: í•™ìŠµí•˜ì§€ ì•Šì€ íŠ¹ì • ë„ë©”ì¸ ë°ì´í„°ë“¤ì— ëŒ€í•œ ì œí•œì  ì§€ì‹
- **í• ë£¨ì‹œë„¤ì´ì…˜**: ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±í•˜ëŠ” ë¬¸ì œ

#### 2-2 RAGì˜ í•´ê²° ë°©ì•ˆ

RAGëŠ” **ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶€ì¡±í•œ ë°ì´í„°ë¥¼ ì£¼ì…**í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•œë‹¤:

- ìµœì‹  ì •ë³´ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€ ìƒì„±
- íŠ¹ì • ë„ë©”ì¸ ì§€ì‹ì˜ ì‹¤ì‹œê°„ í™œìš©
- ì¶œì²˜ê°€ ëª…í™•í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‘ë‹µ ì œê³µ

## 3ï¸âƒ£ RAG ì‘ë™ ê³¼ì •

#### 3-1 ë°ì´í„° ì„ë² ë”© ë° Vector DB êµ¬ì¶•

RAGì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” **ë°ì´í„°ë¥¼ ì„ë² ë”© ëª¨ë¸ì— í†µí•©**í•˜ëŠ” ê²ƒì´ë‹¤.

```python
# ì„ë² ë”© ê³¼ì • ì˜ˆì‹œ
def create_embeddings(text_data):
    """
    í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    """
    # í…ìŠ¤íŠ¸ë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• 
    chunks = chunk_text(text_data, chunk_size=400, chunk_overlap=50)

    # ê° chunkë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    embeddings = []
    for chunk in chunks:
        embedding = embedding_model.encode(chunk)
        embeddings.append(embedding)

    return chunks, embeddings
```

**ì„ë² ë”©**ì´ë€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì˜ ë°ì´í„°ë¥¼ `ë²¡í„° ì„ë² ë”©(Vector Embedding)`ë¼ê³  í•˜ëŠ” ìˆ˜ì¹˜í™”ëœ ë°°ì—´ë¡œ ë³€í™˜í•œ ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤:

- ì‚¬ê³¼ = [0.00212, -0.00328, 0.00789...]
- ë°° = [0.00234, -0.00222, 0.00635...]
- ì»´í“¨í„° = [-0.078, 0.00986, 0.00123...]

### Chunkingì˜ ì´í•´

**Chunking**ì´ë€ ê¸´ ë¬¸ì„œë¥¼ AIì—ê²Œ í†µì§¸ë¡œ ì „ë‹¬í•˜ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê±°ë‚˜ context ê¸¸ì´ ì œí•œì„ ì´ˆê³¼í•´ ì œëŒ€ë¡œ ì²˜ë¦¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆì–´ì„œ, **ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…**ì´ë‹¤.

```python
# Chunking ì˜ˆì‹œ
document = """ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì¸ê³µì§€ëŠ¥ì— ê´€ì‹¬ì´ ë§ê³  ìƒì„±í˜• AIì— ëŒ€í•´ì„œ
ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ RAGë¥¼ ë°°ì› ìŠµë‹ˆë‹¤."""

# Chunk ê²°ê³¼
chunks = [
    "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì¸ê³µì§€ëŠ¥ì— ê´€ì‹¬ì´ ë§ê³  ìƒì„±í˜• AIì— ëŒ€í•´ì„œ ê³µë¶€í•˜",
    "í˜• AIì— ëŒ€í•´ì„œ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ RAGë¥¼ ë°°ì› ìŠµë‹ˆë‹¤."
]
```

**ì£¼ìš” íŒŒë¼ë¯¸í„°:**

- **chunk_size**: í•˜ë‚˜ì˜ chunkì— í¬í•¨ë˜ëŠ” ê¸€ì ìˆ˜
- **chunk_overlap**: ì²­í¬ë¼ë¦¬ ì¼ì • ë¶€ë¶„ ê²¹ì¹˜ê²Œ ë§Œë“œëŠ” ì„¤ì •

`chunk_overlap`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” chunkê°€ ì •í™•íˆ ì˜ë¦¬ëŠ” ì§€ì ì—ì„œ ë¬¸ì¥ì˜ ë§¥ë½ì´ ëŠê¸°ë©´ AIê°€ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›Œì§€ê¸° ë•Œë¬¸ì´ë‹¤.

#### 3-2 ì¿¼ë¦¬ ë²¡í„°í™” ë° ê´€ë ¨ ì •ë³´ ì¶”ì¶œ

ì‚¬ìš©ìì˜ **ì§ˆë¬¸ì„ ë²¡í„°í™”**í•˜ê³ , **ë²¡í„° DBë¥¼ ëŒ€ìƒìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰**ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ìƒìœ„ Kê°œì˜ í•­ëª©ì„ ì¶”ì¶œí•œë‹¤.

##### ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰

ì„ë² ë”©ì„ í†µí•´ ì €ì¥ëœ ë°ì´í„°ëŠ” **ë²¡í„° ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •**í•˜ì—¬ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ í‰ê°€í•œë‹¤. ì£¼ìš” ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

| ë°©ì‹                 | ì„¤ëª…                                     | íŠ¹ì§•                                       |
| -------------------- | ---------------------------------------- | ------------------------------------------ |
| **ìœ í´ë¦¬ë“œ ê±°ë¦¬**    | ë‘ ë²¡í„°ì˜ ëì ì„ ì‡ëŠ” ê°€ì¥ ì§§ì€ ì§ì„ ê±°ë¦¬ | ë²¡í„°ì˜ í¬ê¸°ì™€ ë°©í–¥ì„ ëª¨ë‘ ê³ ë ¤             |
| **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**    | ë‘ ë²¡í„°ê°„ì˜ ê°ë„ë¥¼ ì´ìš©í•˜ì—¬ ì¸¡ì •         | ë²¡í„°ì˜ ë°©í–¥ì— ì¤‘ì , ê³ ì°¨ì› ë°ì´í„°ì— íš¨ê³¼ì  |
| **ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„** | ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ ì¸¡ì •                     | ë°©í–¥ì„±ê³¼ í¬ê¸°ë¥¼ ëª¨ë‘ ê³ ë ¤                  |

```python
def vector_similarity_search(query_vector, database_vectors, top_k=3):
    """
    ì¿¼ë¦¬ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ kê°œ ë²¡í„° ê²€ìƒ‰
    """
    similarities = []

    for i, doc_vector in enumerate(database_vectors):
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(query_vector, doc_vector)
        similarities.append((i, similarity))

    # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]
```

#### 3-3 LLMì„ í†µí•œ ë‹µë³€ ìƒì„±

LLMì€ **ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì¶”ì¶œëœ ê´€ë ¨ ì •ë³´**ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•œë‹¤. ì´ ê³¼ì •ì—ì„œ **ì •í™•í•œ ì¶œì²˜ì— ê¸°ë°˜í•œ ë‹µë³€**ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

```python
def generate_final_answer(query, retrieved_docs):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    context = "\n".join([doc.content for doc in retrieved_docs])

    prompt = f"""
    ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

    ë¬¸ì„œ ë‚´ìš©:
    {context}

    ì§ˆë¬¸: {query}
    ë‹µë³€:
    """

    response = llm.generate(prompt)
    return response
```

## 4ï¸âƒ£ RAG vs Fine-tuning

ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ì„ ëª…í™•íˆ ì´í•´í•´ë³´ì.

#### 4-1 RAG (ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±)

**ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•´ GPTì—ê²Œ ì „ë‹¬**í•˜ê³  ê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤.

#### 4-2 Fine-tuning (ëª¨ë¸ ë¯¸ì„¸ì¡°ì •)

**ê¸°ì¡´ GPT ëª¨ë¸ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í•™ìŠµ**ì‹œì¼œ, ëª¨ë¸ ìì²´ë¥¼ ë°”ê¾¸ëŠ” ë°©ì‹ì´ë‹¤.

#### 4-3 ë¹„êµ ë¶„ì„

| êµ¬ë¶„            | RAG                              | Fine-tuning                             |
| --------------- | -------------------------------- | --------------------------------------- |
| **ë°ì´í„° ë°˜ì˜** | ì™¸ë¶€ ë¬¸ì„œë§Œ ë°”ê¾¸ë©´ ì¦‰ì‹œ ë°˜ì˜     | ìƒˆë¡œ í•™ìŠµí•´ì•¼ ë°˜ì˜ë¨ (ëŠë¦¼)             |
| **êµ¬ì¶• ë³µì¡ë„** | ë¬¸ì„œ ìª¼ê°œê¸° + ë²¡í„°DB êµ¬ì„±        | í•™ìŠµ ë°ì´í„° ì¤€ë¹„ + í•™ìŠµ + íŒŒë¼ë¯¸í„° ì¡°ì • |
| **ë¹„ìš©**        | ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´                  | GPU ë¹„ìš© + í•™ìŠµì‹œê°„ + ëª¨ë¸ í˜¸ìŠ¤íŒ… ë¹„ìš©  |
| **ì‘ë‹µ ì†ë„**   | ê²€ìƒ‰ ë‹¨ê³„ë¡œ ì¸í•´ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ | ëª¨ë¸ ë‚´ì¥ ì§€ì‹ì´ë¼ ë¹ ë¦„                 |
| **ìœ ì—°ì„±**      | ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸ ê°€ëŠ¥        | ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”                        |

```python
# RAG ë°©ì‹ ì˜ˆì‹œ
def rag_approach(query):
    docs = vector_db.search(query)  # ì‹¤ì‹œê°„ ê²€ìƒ‰
    return llm.generate_with_context(query, docs)

# Fine-tuning ë°©ì‹ ì˜ˆì‹œ
def finetuned_approach(query):
    return finetuned_model.generate(query)  # í•™ìŠµëœ ì§€ì‹ í™œìš©
```

## 5ï¸âƒ£ Vector DB

#### 5-1 Vector DBì˜ ì •ì˜

**Vector DB**ëŠ” ë°ì´í„°ë¥¼ **ë²¡í„° í˜•íƒœë¡œ ì €ì¥**í•˜ê³ , **ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰**í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‹¤.

ê¸°ì¡´ì˜ ê´€ê³„í˜• DBê°€ í–‰ê³¼ ì—´ì˜ 2ì°¨ì› í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë°˜ë©´, Vector DBëŠ” **ë‹¤ì°¨ì› ê³µê°„ìœ¼ë¡œ ê°œë…ì„ í™•ì¥**í•œë‹¤.

#### 5-2 Vector DBê°€ í•„ìš”í•œ ì´ìœ 

ìƒì„±í˜• AIë‚˜ RAGì—ì„œ Vector DBë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ :

- ìì—°ì–´ ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
- **AIê°€ ì´í•´ ê°€ëŠ¥í•œ í˜•íƒœì¸ ë²¡í„°ë¡œ ë³€í™˜**í•˜ì—¬ ì €ì¥
- **ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë¬¸ì„œë‚˜ ë¬¸ì¥ë“¤ì„ ë¹ ë¥´ê²Œ ê²€ìƒ‰** ê°€ëŠ¥

```python
# Vector DB ì €ì¥ ë° ê²€ìƒ‰ ì˜ˆì‹œ
class SimpleVectorDB:
    def __init__(self):
        self.vectors = []
        self.documents = []

    def add_document(self, text, vector):
        """ë¬¸ì„œì™€ ë²¡í„°ë¥¼ DBì— ì €ì¥"""
        self.documents.append(text)
        self.vectors.append(vector)

    def search(self, query_vector, top_k=3):
        """ì¿¼ë¦¬ ë²¡í„°ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        similarities = []
        for i, vec in enumerate(self.vectors):
            sim = cosine_similarity(query_vector, vec)
            similarities.append((i, sim))

        # ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = []
        for i, _ in similarities[:top_k]:
            results.append(self.documents[i])

        return results
```

#### 5-3 ì£¼ìš” Vector DB ì¢…ë¥˜

| DB ì´ë¦„      | íŠ¹ì§•                            | ì‚¬ìš© ì¼€ì´ìŠ¤        |
| ------------ | ------------------------------- | ------------------ |
| **Chroma**   | Python ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤, ì„¤ì¹˜ ê°„ë‹¨ | í”„ë¡œí† íƒ€ì…, ì‹¤ìŠµìš© |
| **Pinecone** | í´ë¼ìš°ë“œ ê¸°ë°˜, í™•ì¥ì„± ìš°ìˆ˜      | ìƒìš© ì„œë¹„ìŠ¤        |
| **Weaviate** | GraphQL ì§€ì›, ë‹¤ì–‘í•œ ë²¡í„° ê²€ìƒ‰  | ë³µí•© ê²€ìƒ‰          |
| **Qdrant**   | Rust ê¸°ë°˜, ê³ ì„±ëŠ¥               | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬        |

#### 5-4 Chroma ì„ íƒ ì´ìœ 

ì‹¤ìŠµì—ì„œ **Chroma**ë¥¼ ì„ íƒí•œ ì´ìœ :

- Python ê¸°ë°˜ì˜ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ **ì„¤ì¹˜ ê³¼ì •ì´ ë‹¨ìˆœ**
- ë³„ë„ì˜ ì„œë²„ë¥¼ ë„ìš°ê±°ë‚˜ ë³µì¡í•œ ì„¤ì • ì—†ì´ **ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ë²¡í„° DB êµ¬ì¶• ê°€ëŠ¥**
- ë¹ ë¥´ê²Œ ê°œë…ì„ ìµíˆê³  ê°„ë‹¨í•œ í™˜ê²½ì—ì„œ ì§ì ‘ ì²´í—˜í•˜ê¸°ì— ì í•©

```bash
# Chroma ì„¤ì¹˜
pip install chromadb
```

## 6ï¸âƒ£ ì‹¤ìŠµì½”ë“œ 1 - Vector DB êµ¬ì¶•

#### 6-1 ì „ì²´ íë¦„ ìš”ì•½

- **ì´ˆê¸°í™”**: ë°ì´í„°ë² ì´ìŠ¤ì™€ ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•˜ì—¬ ì˜êµ¬ ì €ì¥ì†Œ ì¤€ë¹„
- **ë°ì´í„° ë¡œë“œ**: ì§€ì •ëœ í´ë”ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ë¶ˆëŸ¬ì˜´
- **ì „ì²˜ë¦¬**: ê° í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì¼ì • ê¸¸ì´ì˜ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€
- **ì„ë² ë”© ìƒì„± ë° ì €ì¥**: ê° ì²­í¬ì— ëŒ€í•´ OpenAI ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë²¡í„° DBì— ì €ì¥

#### 6-2 build_vector_db.py êµ¬í˜„

##### í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”

```python
import os
from openai import OpenAI
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œí•˜ì—¬ API í‚¤ ê°€ì ¸ì˜¤ê³  OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
```

##### DB ì´ˆê¸°í™” í•¨ìˆ˜

```python
def init_db(db_path="./chroma_db"):
    """Vector DB ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±"""
    dbclient = chromadb.PersistentClient(path=db_path)
    # rag_collectionì´ë¼ëŠ” ë°ì´í„° ì»¬ë ‰ì…˜ ìƒì„±
    collection = dbclient.create_collection(
        name="rag_collection",
        get_or_create=True
    )
    return dbclient, collection
```

##### í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©

```python
def load_text_files(folder_path):
    """ì§€ì •ëœ í´ë”ì—ì„œ ëª¨ë“  .txt íŒŒì¼ ë¡œë“œ"""
    docs = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if file_path.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
                docs.append((filename, text))
    return docs
```

##### ì„ë² ë”© ìƒì„±

```python
def get_embedding(text, model="text-embedding-3-large"):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    response = client.embeddings.create(input=[text], model=model)
    embedding = response.data[0].embedding
    return embedding
```

##### í…ìŠ¤íŠ¸ ì²­í‚¹

```python
def chunk_text(text, chunk_size=400, chunk_overlap=50):
    """ì›ì²œ ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  overlap ì ìš©"""
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - chunk_overlap

        # ì˜ˆì™¸ ì²˜ë¦¬
        if start < 0:
            start = 0
        if start >= len(text):
            break

    return chunks
```

##### ë©”ì¸ ì‹¤í–‰ ë¡œì§

```python
if __name__ == "__main__":
    # DB ì´ˆê¸°í™”
    dbclient, collection = init_db("./chroma_db")

    # ë¬¸ì„œ ë¡œë“œ
    folder_path = "./source_data"
    docs = load_text_files(folder_path)

    doc_id = 0
    for filename, text in docs:
        chunks = chunk_text(text, chunk_size=400, chunk_overlap=50)

        for idx, chunk in enumerate(chunks):
            doc_id += 1
            embedding = get_embedding(chunk)

            # Vector DBì— ì €ì¥
            collection.add(
                documents=[chunk],  # ì‹¤ì œ ì²­í¬ í…ìŠ¤íŠ¸
                embeddings=[embedding],  # ìƒì„±ëœ ì„ë² ë”© ë²¡í„°
                metadatas=[{
                    "filename": filename,
                    "chunk_index": idx
                }],
                ids=[str(doc_id)]  # ê³ ìœ  ì‹ë³„ì
            )

    print("ëª¨ë“  ë¬¸ì„œê°€ Vector DBì— ì €ì¥ ì™„ë£Œ")
```

## 7ï¸âƒ£ ì‹¤ìŠµì½”ë“œ 2 - RAG ì±—ë´‡ êµ¬í˜„

#### 7-1 rag_chatbot.py êµ¬í˜„

##### í™˜ê²½ ì„¤ì • ë° DB ì—°ê²°

```python
import os
from openai import OpenAI
from build_vector_db import get_embedding
import chromadb
from dotenv import load_dotenv

load_dotenv()
dbclient = chromadb.PersistentClient(path="./chroma_db")
collection = dbclient.get_or_create_collection("rag_collection")
```

##### ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜

```python
def retrieve(query, top_k=3):
    """ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ top-kê°œ ë¬¸ì„œ ê²€ìƒ‰"""
    query_embedding = get_embedding(query)

    # Vector DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )

    return results
```

##### RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±

```python
def generate_answer_with_context(query, top_k=3):
    """
    RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± í”„ë¡œì„¸ìŠ¤:
    1) ì¿¼ë¦¬ì— ëŒ€í•´ Vector DBì—ì„œ top_kê°œ ë¬¸ì„œ ê²€ìƒ‰
    2) ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ contextë¡œ êµ¬ì„±
    3) Contextì™€ í•¨ê»˜ GPTì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬
    4) ìµœì¢… ë‹µë³€ ë°˜í™˜
    """

    # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    results = retrieve(query, top_k)
    found_docs = results["documents"][0]
    found_metadatas = results["metadatas"][0]

    # 2. Context êµ¬ì„±
    context_texts = []
    for doc_text, meta in zip(found_docs, found_metadatas):
        context_texts.append(
            f"<<filename: {meta['filename']}>>\n{doc_text}"
        )
    context_str = "\n\n".join(context_texts)

    # 3. í”„ë¡¬í”„íŠ¸ ì‘ì„±
    system_prompt = """
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”
    ì§€ëŠ¥í˜• ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì›ì¹™ì„ ì—„ê²©íˆ ì§€í‚¤ì„¸ìš”:

    1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    2. ë¬¸ì„œì— ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì´ë¼ë©´, í•¨ë¶€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    3. ì‚¬ì‹¤ ê´€ê³„ë¥¼ ëª…í™•íˆ ê¸°ìˆ í•˜ê³ , ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
    4. ì§€ë‚˜ì¹˜ê²Œ ì¥í™©í•˜ì§€ ì•Šê²Œ, ê°„ê²°í•˜ê³  ì•Œê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    5. ë¬¸ì„œ ì¶œì²˜ë‚˜ ì—°ë„ê°€ ì¤‘ìš”í•˜ë‹¤ë©´, ê°€ëŠ¥í•œ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
    """

    user_prompt = f"""ì•„ë˜ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤:
    {context_str}
    ì§ˆë¬¸: {query}"""

    # 4. ChatGPT í˜¸ì¶œ
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )

    answer = response.choices[0].message.content
    return answer
```

##### ë¹„êµìš© ì¼ë°˜ GPT ë‹µë³€

```python
def generate_answer_without_context(query):
    """RAG ì—†ì´ ì¼ë°˜ GPTë¡œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜"""
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": query}
        ]
    )

    answer = response.choices[0].message.content
    return answer
```

##### ë©”ì¸ ì‹¤í–‰ ë£¨í”„

```python
if __name__ == "__main__":
    while True:
        user_query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”(ì¢…ë£Œ: quit): ")
        if user_query.lower() == "quit":
            break

        # RAG ê¸°ë°˜ ë‹µë³€
        answer = generate_answer_with_context(user_query, top_k=3)
        # ì¼ë°˜ GPT ë‹µë³€ (ë¹„êµìš©)
        # answer = generate_answer_without_context(user_query)

        print("===ë‹µë³€===")
        print(answer)
        print("==========\n")
```

#### 7-2 ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°

```python
# collection.query() ê²°ê³¼ ì˜ˆì‹œ
{
  "documents": [
    [ë¬¸ì„œ1, ë¬¸ì„œ2, ë¬¸ì„œ3]  # í•œ ê°œì˜ ì¿¼ë¦¬ì— ëŒ€í•œ top_k ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
  ],
  "metadatas": [
    [ë©”íƒ€ë°ì´í„°1, ë©”íƒ€ë°ì´í„°2, ë©”íƒ€ë°ì´í„°3]  # ê° ë¬¸ì„œì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
  ]
}
```

## âœ… ì •ë¦¬

**RAGì˜ í•µì‹¬ í¬ì¸íŠ¸**

- ğŸ” **ê²€ìƒ‰ ê¸°ë°˜**: ì™¸ë¶€ ì§€ì‹ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ í™œìš©
- ğŸ“Š **ë²¡í„° ê²€ìƒ‰**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ì˜ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰
- ğŸ¯ **ë§¥ë½ ì œê³µ**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë§¥ë½ìœ¼ë¡œ í™œìš©í•œ ì •í™•í•œ ë‹µë³€ ìƒì„±
- âš¡ **ì‹¤ì‹œê°„ ë°˜ì˜**: ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥

**RAG vs Fine-tuning ì„ íƒ ê¸°ì¤€**

- **RAG**: ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸, ë¹„ìš© íš¨ìœ¨ì„±, ì¶œì²˜ ëª…ì‹œê°€ ì¤‘ìš”í•œ ê²½ìš°
- **Fine-tuning**: íŠ¹ì • ìŠ¤íƒ€ì¼/í†¤ í•™ìŠµ, ì‘ë‹µ ì†ë„, ì¼ê´€ëœ í–‰ë™ íŒ¨í„´ì´ ì¤‘ìš”í•œ ê²½ìš°

**êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­**

- **ì²­í‚¹ ì „ëµ**: chunk_sizeì™€ overlap ìµœì í™”
- **ë²¡í„° DB ì„ íƒ**: ìš©ëŸ‰ê³¼ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” DB ì„ íƒ
- **ì„ë² ë”© ëª¨ë¸**: ë„ë©”ì¸ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸ í™œìš©
- **ê²€ìƒ‰ ìµœì í™”**: ìœ ì‚¬ë„ ê¸°ì¤€ê³¼ ìƒìœ„ Kê°œ ê²°ê³¼ ìˆ˜ ì¡°ì •

RAGëŠ” í˜„ì¬ ìƒì„±í˜• AIì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì´ë‹¤. ì²´ê³„ì ì¸ ì´í•´ì™€ ì‹¤ìŠµì„ í†µí•´ ì‹¤ë¬´ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.
