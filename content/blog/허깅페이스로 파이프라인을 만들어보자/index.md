---
title: "í—ˆê¹…í˜ì´ìŠ¤ë¡œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ë³´ì - íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í™œìš© ê°€ì´ë“œ"
slug: "make-huggingface-pipeline"
date: "2025-10-04"
description: "í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ NLP ëª¨ë¸ì„ ì‰½ê²Œ í™œìš©í•˜ëŠ” ë°©ë²•ë¶€í„° ë³µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë³´ì."
thumbnail: "./index.png"
pointColor: "#FF9A00"
tags: ["AI"]
keywords: "í—ˆê¹…í˜ì´ìŠ¤, Hugging Face, íŠ¸ëœìŠ¤í¬ë¨¸, Transformer, NLP, ìì—°ì–´ ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹, AI, Python, íŒŒì´í”„ë¼ì¸, BERT, GPT, ë”¥ëŸ¬ë‹"
---

![ã„±ã…‡ã…‡](./index.png)

> ìˆ˜ë§ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ìŸì•„ì ¸ ë‚˜ì˜¤ì§€ë§Œ ê°ê° ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ìœ¼ë¡œ ì¸í•´ í™œìš©ì´ ì–´ë ¤ì› ë‹¤. í—ˆê¹…í˜ì´ìŠ¤ëŠ” í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë³µì¡í•œ NLP íŒŒì´í”„ë¼ì¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

## 0ï¸âƒ£ êµ¬ê¸€ ì½”ë©ê³¼ ê¹ƒí—ˆë¸Œ ì—°ë™í•˜ê¸°

#### 0-1 ê°œë°œ í™˜ê²½ ì¤€ë¹„

ì‹¤ìŠµì„ ìœ„í•´ êµ¬ê¸€ ì½”ë©(Google Colab)ê³¼ ê¹ƒí—ˆë¸Œ(Github) ì—°ë™ì´ í•„ìš”í•˜ë‹¤.

```python
# GPU ì‚¬ìš© ì„¤ì • í™•ì¸
import torch
device = 0 if torch.cuda.is_available() else -1
print(f"Using device: {'GPU' if device == 0 else 'CPU'}")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers datasets torch pandas
```

## 1ï¸âƒ£ í—ˆê¹…í˜ì´ìŠ¤ ê°œìš”

#### 1-1 ë“±ì¥ ë°°ê²½

**ğŸ’¡ ë¬¸ì œì **

- íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ë“¤ì´ ê¸‰ì†íˆ ì¦ê°€ â¬†
- ëª¨ë¸ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ê³¼ ì¸í„°í˜ì´ìŠ¤ ì¡´ì¬
- ìƒˆë¡œìš´ ëª¨ë¸ ì‚¬ìš©ë²•ì„ ë§¤ë²ˆ ìƒˆë¡œ ìµí˜€ì•¼ í•˜ëŠ” ì–´ë ¤ì›€

**âœ… í•´ê²° ë°©ì•ˆ**

ê³µí†µëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œë°œ â†’ `Huggingface` íŒ€ì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬

#### 1-2 í—ˆê¹…í˜ì´ìŠ¤ë€?

ë‹¤ì–‘í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ **í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤**ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì§€ì›í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬/í”Œë«í¼ì´ë‹¤.

**ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì„±**

- `transformers`: ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì§€ì›
- `datasets`: ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë° ë‹¤ìš´ë¡œë“œ ì§€ì›
- ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ í™œìš© ê°€ëŠ¥

```python
# í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ ì˜ˆì‹œ
text = "What is Huggingface Transformers?"

# BERT ëª¨ë¸ í™œìš©
bert_model = AutoModel.from_pretrained("bert-base-uncased")
bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
encoded_input = bert_tokenizer(text, return_tensors='pt')
bert_output = bert_model(**encoded_input)

# GPT-2 ëª¨ë¸ í™œìš©
gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')
gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')
encoded_input = gpt_tokenizer(text, return_tensors='pt')
gpt_output = gpt_model(**encoded_input)
```

**í”Œë«í¼(Hub)**

- `huggingface_hub`: ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ íƒìƒ‰ ë° ê³µìœ  ì§€ì›
- https://huggingface.co/

#### 1-3 í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œ íƒìƒ‰í•˜ê¸°

##### ëª¨ë¸ í—ˆë¸Œ

- https://huggingface.co/models

**í•„í„°ë§ ì˜µì…˜**

| ì¹´í…Œê³ ë¦¬      | ì„¤ëª…                     | ì˜ˆì‹œ                              |
| ------------- | ------------------------ | --------------------------------- |
| **Tasks**     | ì‘ì—… ì¢…ë¥˜ë³„ í•„í„°ë§       | NLP, CV, Audio, Multimodal        |
| **Libraries** | í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ í•„í„°ë§ | PyTorch, TensorFlow, Transformers |
| **Languages** | ì§€ì› ì–¸ì–´ë³„ í•„í„°ë§       | í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´              |

**ì£¼ìš” Task ìœ í˜•**

| Task           | ì„¤ëª…          | ëª¨ë¸ ì˜ˆì‹œ                   |
| -------------- | ------------- | --------------------------- |
| **NLP**        | ìì—°ì–´ ì²˜ë¦¬   | ë²ˆì—­, ìš”ì•½, ê°ì • ë¶„ì„ ëª¨ë¸  |
| **CV**         | ì»´í“¨í„° ë¹„ì „   | ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€ ëª¨ë¸ |
| **Audio**      | ì˜¤ë””ì˜¤ ì²˜ë¦¬   | ìŒì„± ì¸ì‹, TTS              |
| **Multimodal** | ë³µí•© ëª¨ë‹¬ë¦¬í‹° | ì´ë¯¸ì§€ ìº¡ì…”ë‹, VQA ëª¨ë¸     |

##### ë°ì´í„°ì…‹ í—ˆë¸Œ

https://huggingface.co/datasets

**KLUE Dataset ì˜ˆì‹œ**

- **Subset**: ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°ê° ë‹¤ë¥¸ í•™ìŠµ ëª©ì 
- **Split**: Train/Validation 8:2 ë¹„ìœ¨ë¡œ êµ¬ì„±

> **Train**: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° í™œìš©ëœë‹¤.
>
> **Validation**: ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, í•™ìŠµ ê³¼ì •ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.

## 2ï¸âƒ£ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í™œìš©í•˜ê¸°

#### 2-1 ëª¨ë¸ êµ¬ì¡°: ë°”ë”” + í—¤ë“œ

|                    ë°”ë””                    |                    í—¤ë“œ                     |
| :----------------------------------------: | :-----------------------------------------: |
| ëª¨ë¸ì˜ ì¤‘ì‹¬ ì—”ì§„ìœ¼ë¡œ ì¶”ë¡  ëª¨ë¸ì˜ í•µì‹¬ ë¶€ë¶„ | íŠ¹ì • íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê¸°ëŠ¥ë³„ ì„¸ë¶„í™”ëœ ë¶€ë¶„ |

í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë™ì¼í•œ ë°”ë””ì— ì„œë¡œ ë‹¤ë¥¸ í—¤ë“œë¥¼ ë¶™ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

#### 2-2 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

**1. ë°”ë””ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°**

```python
from transformers import AutoModel

base_model_id = 'bert-base-uncased'
base_model = AutoModel.from_pretrained(base_model_id)
# ì¶œë ¥: hidden state(ê³ ì°¨ì› ë²¡í„°)
# ëª©ì : ëª¨ë¸ì„ feature extractor(íŠ¹ì§• ì¶”ì¶œê¸°)ë¡œ í™œìš©
```

**2. ë¶„ë¥˜ í—¤ë“œê°€ í¬í•¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°**

```python
from transformers import AutoModelForSequenceClassification

model_id = 'SamLowe/roberta-base-go_emotions'
classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)
# ê°ì • ë¶„ë¥˜ê°€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸
```

**3. ë¶„ë¥˜ í—¤ë“œê°€ ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°**

```python
from transformers import AutoModelForSequenceClassification

model_id = 'klue/roberta-base'
random_model = AutoModelForSequenceClassification.from_pretrained(model_id)
# ë°”ë””: ì‚¬ì „í•™ìŠµ ì™„ë£Œ, í—¤ë“œ: ëœë¤ ì´ˆê¸°í™”
# ëª©ì : íŠ¹ì • taskì— ë§ì¶° fine-tuningí•  ë•Œ ì‚¬ìš©
```

#### 2-3 í† í¬ë‚˜ì´ì € í™œìš©í•˜ê¸°

**í† í¬ë‚˜ì´ì €ì˜ ì—­í• **

í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê° í† í°ì„ ëŒ€ì‘í•˜ëŠ” **í† í° ID**ë¡œ ë³€í™˜í•˜ëŠ” ì•„í‚¤í…ì²˜ë‹¤.

- **í† í°**: ì–¸ì–´ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„
- í•™ìŠµ ë°ì´í„°ë¥¼ í†µí•´ í† í° ì‚¬ì „ êµ¬ì¶• â†’ ëª¨ë¸ë§ˆë‹¤ í† í¬ë‚˜ì´ì§• ë°©ë²•ì´ ë‹¤ë¦„
- **ë™ì¼í•œ ëª¨ë¸ ID**ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í†µì¼í•´ì•¼ í•¨

```python
from transformers import AutoTokenizer

model_id = 'klue/roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

**í† í¬ë‚˜ì´ì € ì¶œë ¥ê°’**

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "I love banana"
encoded = tokenizer(text, padding=True, truncation=True)

print(encoded)
# ì¶œë ¥ ì˜ˆì‹œ:
# {
#  'input_ids': [101, 1045, 2293, 15212, 102],
#  'token_type_ids': [0, 0, 0, 0, 0],
#  'attention_mask': [1, 1, 1, 1, 1]
# }
```

**ì£¼ìš” ì¶œë ¥ ìš”ì†Œ**

- `input_ids`: ê° í† í°ì˜ ì‚¬ì „ ë‚´ ì¸ë±ìŠ¤ ë²ˆí˜¸
- `token_type_ids`: í† í°ì´ ì†í•œ ë¬¸ì¥ ID (ì²« ë²ˆì§¸ ë¬¸ì¥: 0, ë‘ ë²ˆì§¸ ë¬¸ì¥: 1)
- `attention_mask`: padding token ì—¬ë¶€ (ì‹¤ì œ í† í°: 1, padding: 0)

**í† í° í™•ì¸í•˜ê¸°**

```python
tokenized = tokenizer("í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤")

# í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))
# ['[CLS]', 'í† í¬', '##ë‚˜ì´', '##ì €', '##ëŠ”', 'í…ìŠ¤íŠ¸', '##ë¥¼', 'í† ', '##í°', 'ë‹¨ìœ„', '##ë¡œ', 'ë‚˜ëˆˆë‹¤', '[SEP]']

# ì›ë˜ ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©
print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))
# í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤
```

**ë°°ì¹˜ ë””ì½”ë”©**

```python
# ê°œë³„ ë¬¸ì¥ë“¤ì˜ ë°°ì¹˜ ì²˜ë¦¬
first_tokenized = tokenizer(['ì²« ë²ˆì§¸ ë¬¸ì¥', 'ë‘ ë²ˆì§¸ ë¬¸ì¥'])['input_ids']
print(tokenizer.batch_decode(first_tokenized))
# ['[CLS] ì²« ë²ˆì§¸ ë¬¸ì¥ [SEP]', '[CLS] ë‘ ë²ˆì§¸ ë¬¸ì¥ [SEP]']

# ë¬¸ì¥ ìŒìœ¼ë¡œ ì²˜ë¦¬
second_tokenized = tokenizer([['ì²« ë²ˆì§¸ ë¬¸ì¥', 'ë‘ ë²ˆì§¸ ë¬¸ì¥']])['input_ids']
print(tokenizer.batch_decode(second_tokenized))
# ['[CLS] ì²« ë²ˆì§¸ ë¬¸ì¥ [SEP] ë‘ ë²ˆì§¸ ë¬¸ì¥ [SEP]']
```

## 3ï¸âƒ£ ë°ì´í„°ì…‹ í™œìš©í•˜ê¸°

#### 3-1 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

**í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ**

```python
from datasets import load_dataset

# MRC(Machine Reading Comprehension) ë°ì´í„°ì…‹
klue_mrc_dataset = load_dataset('klue', 'mrc')
klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')
```

**ë¡œì»¬ ë°ì´í„° í™œìš©**

```python
# JSON íŒŒì¼ ë¡œë“œ
dataset_json = load_dataset("json", data_files="/path/to/data.json")

# ìˆ˜ë™ìœ¼ë¡œ train/test ë¶„í• 
dataset_json_test = dataset_json["train"].train_test_split(test_size=0.2, seed=42)["test"]
```

**Python ë”•ì…”ë„ˆë¦¬/DataFrame í™œìš©**

```python
from datasets import Dataset
import pandas as pd

# ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)

# pandas DataFrameì—ì„œ ìƒì„±
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)
```

#### 3-2 ë°ì´í„°ì…‹ ê°€ê³µí•˜ê¸°

**KLUE YNAT ë°ì´í„°ì…‹ ì˜ˆì‹œ**

```python
from datasets import load_dataset

klue_tc_train = load_dataset("klue", "ynat", split="train")
klue_tc_eval = load_dataset("klue", "ynat", split="validation")

# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
print(klue_tc_train)
# Dataset({
#     features: ['guid', 'title', 'label', 'url', 'date'],
#     num_rows: 45678
# })
```

**ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°**

```python
klue_tc_train_removed = klue_tc_train.remove_columns(['guid', 'url', 'date'])
```

**ì»¬ëŸ¼ ì¶”ê°€í•˜ê¸°**

```python
# labelì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
klue_tc_label = klue_tc_train_removed.features['label']

def make_str_label(batch):
    batch['label_str'] = klue_tc_label.int2str(batch['label'])
    return batch


# mapì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
klue_tc_train_removed = klue_tc_train_removed.map(
    make_str_label,
    batched=True,
    batch_size=1000
)
```

> **ğŸ’¡ map í•¨ìˆ˜ë€?**
>
> ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìš”ì†Œì— ëŒ€í•´ ë™ì¼í•œ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” í—ˆê¹…í˜ì´ìŠ¤ datasetsì˜ í•µì‹¬ ë©”ì„œë“œë‹¤.
>
> - **batched=True**: ì—¬ëŸ¬ ìƒ˜í”Œì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
> - **batch_size**: í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ ì§€ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
> - **num_proc**: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›

**Train/Validation/Test ë¶„í• **

```python
# í•™ìŠµìš© ë°ì´í„° 1ë§Œê°œ ì¶”ì¶œ
train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']

# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° 1000ê°œ ì¶”ì¶œ
dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)
test_dataset = dataset['test']

# ê²€ì¦ìš© ë°ì´í„° 1000ê°œ ì¶”ì¶œ
valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']
```

## 4ï¸âƒ£ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì¶”ë¡ í•˜ê¸°

#### 4-1 BERT vs GPT ëª¨ë¸ ë¹„êµ

| íŠ¹ì§•              | BERT                        | GPT                      |
| ----------------- | --------------------------- | ------------------------ |
| **ì•„í‚¤í…ì²˜**      | íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë§Œ ì‚¬ìš©    | íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ë§Œ ì‚¬ìš© |
| **ë°©í–¥ì„±**        | ì–‘ë°©í–¥(Bidirectional)       | ë‹¨ë°©í–¥(ì™¼ìª½â†’ì˜¤ë¥¸ìª½)      |
| **ì‚¬ì „í•™ìŠµ ëª©í‘œ** | MLM + NSP                   | í‘œì¤€ ì–¸ì–´ ëª¨ë¸ë§         |
| **ì£¼ìš” ê°•ì **     | í…ìŠ¤íŠ¸ ì´í•´ ë° ë¶„ì„         | í…ìŠ¤íŠ¸ ìƒì„±              |
| **í™œìš© ë¶„ì•¼**     | ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ì§ˆì˜ì‘ë‹µ | í…ìŠ¤íŠ¸ ìƒì„±, ëŒ€í™”í˜• AI   |
| **í™œìš© ë°©ì‹**     | Fine-tuning ì¤‘ì‹¬            | Zero-shot, Few-shot í•™ìŠµ |

> **ğŸ’¡ ì¸ì½”ë”ì™€ ë””ì½”ë”**
>
> **ì¸ì½”ë”(Encoder)**: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ê³  ë¬¸ë§¥ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì—­í• 
>
> - ì…ë ¥ ì „ì²´ë¥¼ í•œ ë²ˆì— ë³´ê³  ì–‘ë°©í–¥ìœ¼ë¡œ ì •ë³´ ì²˜ë¦¬
> - í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ë° íŠ¹í™”
>
> **ë””ì½”ë”(Decoder)**: ìˆœì°¨ì ìœ¼ë¡œ í† í°ì„ ìƒì„±í•˜ëŠ” ì—­í• 
>
> - ì´ì „ í† í°ë“¤ë§Œ ì°¸ì¡°í•˜ì—¬ ë‹¤ìŒ í† í° ì˜ˆì¸¡
> - ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„±ì— íŠ¹í™”

#### 4-2 BERT ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ê¸°

##### íŒŒì´í”„ë¼ì¸ í™œìš©

```python
from transformers import pipeline

model_id = "hykiim/roberta-base-klue-ynat-classification"
model_pipeline = pipeline("text-classification", model=model_id)

# í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤í–‰
results = model_pipeline(test_dataset["title"][:5])
print(results)
```

**ì£¼ìš” íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜**

| íŒŒì´í”„ë¼ì¸             | ì‘ì—…        | ì˜ˆì‹œ                   |
| ---------------------- | ----------- | ---------------------- |
| `text-classification`  | í…ìŠ¤íŠ¸ ë¶„ë¥˜ | ê°ì • ë¶„ì„, ìŠ¤íŒ¸ ê°ì§€   |
| `token-classification` | í† í° ë¶„ë¥˜   | ê°œì²´ëª… ì¸ì‹, í’ˆì‚¬ íƒœê¹… |
| `text-generation`      | í…ìŠ¤íŠ¸ ìƒì„± | ê¸€ì“°ê¸°, ì½”ë”©           |
| `question-answering`   | ì§ˆì˜ì‘ë‹µ    | ë…í•´, FAQ ì±—ë´‡         |
| `summarization`        | ìš”ì•½        | ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½         |
| `translation`          | ë²ˆì—­        | ë‹¤êµ­ì–´ ë²ˆì—­            |

##### ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

```python
import torch
from torch.nn.functional import softmax
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class CustomPipeline:
    def __init__(self, model_id):
        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model.eval()  # í‰ê°€ ëª¨ë“œ

    def __call__(self, texts):
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True
        )

        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = self.model(**tokenized)
            logits = outputs.logits

        # í™•ë¥  ê³„ì‚° ë° ì˜ˆì¸¡
        probabilities = softmax(logits, dim=-1)
        scores, labels = torch.max(probabilities, dim=-1)
        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]

        return [{"label": label, "score": score.item()} for label, score in zip(labels_str, scores)]

# ì‚¬ìš© ì˜ˆì‹œ
custom_pipeline = CustomPipeline(model_id)
results = custom_pipeline(test_dataset['title'][:5])
```

#### 4-3 GPT ëª¨ë¸ë¡œ ë¬¸ì¥ ì´ì–´ì“°ê¸°

##### íŒŒì´í”„ë¼ì¸ ë²„ì „

```python
from transformers import pipeline
import torch

# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
generator = pipeline('text-generation', model='gpt2', device=0)

prompt = "Once upon a time, in a land far, far away,"

# í…ìŠ¤íŠ¸ ìƒì„±
results = generator(
    prompt,
    max_length=100,
    num_return_sequences=3,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    no_repeat_ngram_size=2
)

for i, result in enumerate(results):
    print(f"ê²°ê³¼ {i+1}: {result['generated_text']}")
```

##### ì§ì ‘ êµ¬í˜„ ë²„ì „

```python
from transformers import GPT2LMHeadModel, AutoTokenizer
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')
gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')

# pad í† í° ì„¤ì •
if gpt_tokenizer.pad_token is None:
    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token

# í…ìŠ¤íŠ¸ ìƒì„±
text = "What is Huggingface Transformer?"
encoded_input = gpt_tokenizer(text, return_tensors='pt')

output_sequences = gpt_model.generate(
    input_ids=encoded_input['input_ids'],
    max_length=50,
    num_return_sequences=1,
    pad_token_id=gpt_tokenizer.pad_token_id,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)

# ê²°ê³¼ ë””ì½”ë”©
generated_text = gpt_tokenizer.decode(output_sequences[0], skip_special_tokens=True)
print("Generated Text:", generated_text)
```

## 5ï¸âƒ£ ë‰´ìŠ¤ê¸°ì‚¬ ìš”ì•½â†’ë²ˆì—­â†’ê°ì •ë¶„ì„ íŒŒì´í”„ë¼ì¸

#### 5-1 íŒŒì´í”„ë¼ì¸ ì„¤ê³„

**ì „ì²´ ì›Œí¬í”Œë¡œìš°**

1. ê¸°ì‚¬ ë‚´ìš©(í•œêµ­ì–´) â†’ ìš”ì•½(í•œêµ­ì–´)
2. ìš”ì•½(í•œêµ­ì–´) â†’ ë²ˆì—­(ì˜ì–´)
3. ë²ˆì—­(ì˜ì–´) â†’ ê°ì •ë¶„ì„

#### 5-2 ë°ì´í„°ì…‹ ì¤€ë¹„

```python
from datasets import load_dataset
import pandas as pd

# KLUE MRC ë°ì´í„°ì…‹ ë¡œë“œ
full_dataset = load_dataset("klue", "mrc", split="train")

# ì‹¤ìŠµìš© ë°ì´í„° ì¼ë¶€ ì„ íƒ
num_samples_to_use = 10
klue_mrc_subset = full_dataset.select(range(num_samples_to_use))

print("ë¡œë“œëœ ë°ì´í„°ì…‹ ì •ë³´:")
print(klue_mrc_subset)
```

#### 5-3 1ë‹¨ê³„: ê¸°ì‚¬ ìš”ì•½

```python
# ìš”ì•½ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
summarizer = pipeline(
    task="summarization",
    model="gogamza/kobart-summarization",
    device=device
)

# ìš”ì•½ í•¨ìˆ˜ ì •ì˜
def summarize_context(example):
    summary_result = summarizer(
        example['context'],
        max_length=150,
        min_length=30,
        do_sample=False
    )
    example['summary'] = summary_result[0]['summary_text']
    return example

# ë°ì´í„°ì…‹ì— ìš”ì•½ ì ìš©
print("ìš”ì•½ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
summarized_dataset = klue_mrc_subset.map(summarize_context)
print("ìš”ì•½ ì‘ì—… ì™„ë£Œ.")
```

#### 5-4 2ë‹¨ê³„: í•œì˜ ë²ˆì—­

```python
# ë²ˆì—­ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
translator = pipeline(
    task="translation",
    model="facebook/nllb-200-distilled-600M",
    device=device
)

# ë²ˆì—­ í•¨ìˆ˜ ì •ì˜
def translate_summary_to_english(example):
    translation_result = translator(
        example['summary'],
        src_lang="kor_Hang",
        tgt_lang="eng_Latn",
        max_length=150,
        min_length=30,
        do_sample=False
    )
    example['english_summary'] = translation_result[0]['translation_text']
    return example

# ë°ì´í„°ì…‹ì— ë²ˆì—­ ì ìš©
print("ë²ˆì—­ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
translated_dataset = summarized_dataset.map(translate_summary_to_english)
print("ë²ˆì—­ ì‘ì—… ì™„ë£Œ.")
```

#### 5-5 3ë‹¨ê³„: ê°ì • ë¶„ì„

```python
# ê°ì • ë¶„ì„ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
emotion_classifier = pipeline(
    task="text-classification",
    model="SamLowe/roberta-base-go_emotions",
    top_k=1,
    device=device
)

# ê°ì • ë¶„ì„ í•¨ìˆ˜ ì •ì˜
def analyze_emotion(example):
    emotion_result = emotion_classifier(example['english_summary'])
    example['emotion'] = emotion_result[0][0]['label']
    return example

# ë°ì´í„°ì…‹ì— ê°ì • ë¶„ì„ ì ìš©
print("ê°ì • ë¶„ì„ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
final_dataset = translated_dataset.map(analyze_emotion)
print("ê°ì • ë¶„ì„ ì‘ì—… ì™„ë£Œ.")
```

#### 5-6 ê²°ê³¼ í™•ì¸

```python
print("ìµœì¢… ë°ì´í„°ì…‹ ì»¬ëŸ¼:", final_dataset.column_names)

# ìµœì¢… ê²°ê³¼ í™•ì¸
for i in range(min(5, len(final_dataset))):
    print(f"\n--- ìƒ˜í”Œ {i+1} ---")
    print(f"ì›ë¬¸ ì¼ë¶€: {final_dataset[i]['context'][:100]}...")
    print(f"ìš”ì•½: {final_dataset[i]['summary']}")
    print(f"ì˜ì–´ ë²ˆì—­: {final_dataset[i]['english_summary']}")
    print(f"ê°ì • ë¶„ì„: {final_dataset[i]['emotion']}")

# CSVë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
df_final = pd.DataFrame(final_dataset)
df_final.to_csv('news_analysis_pipeline_results.csv', index=False)
```
