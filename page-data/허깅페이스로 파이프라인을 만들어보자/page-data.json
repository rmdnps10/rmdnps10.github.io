{"componentChunkName":"component---src-templates-blog-post-js","path":"/허깅페이스로 파이프라인을 만들어보자/","result":{"data":{"site":{"siteMetadata":{"title":"난너의오른팔🧑‍💻"}},"markdownRemark":{"id":"4f2ebd57-1b6b-5267-9012-58a0c9529d93","excerpt":"수많은 트랜스포머 모델이 쏟아져 나오지만 각각 다른 구현 방식으로 인해 활용이 어려웠다. 허깅페이스는 통일된 인터페이스로 이 문제를 해결하고, 복잡한 NLP 파이프라인을 간단하게 구축할 수 있게 해준다. 0️⃣ 구글 코랩과 깃허브 연동하기 0-…","html":"<p><figure class=\"gatsby-resp-image-figure\" style=\"background: transparent;\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.400000000000006%; position: relative; bottom: 0; left: 0; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/1b3d79daf52aff8152fd2b54c697b855/bc904/index.webp 250w,\n/static/1b3d79daf52aff8152fd2b54c697b855/4be29/index.webp 500w,\n/static/1b3d79daf52aff8152fd2b54c697b855/f8b1b/index.webp 600w\"\n              sizes=\"(max-width: 600px) 100vw, 600px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/1b3d79daf52aff8152fd2b54c697b855/43fa5/index.png 250w,\n/static/1b3d79daf52aff8152fd2b54c697b855/c6e3d/index.png 500w,\n/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png 600w\"\n            sizes=\"(max-width: 600px) 100vw, 600px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png\"\n            alt=\"ㄱㅇㅇ\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">ㄱㅇㅇ</figcaption>\n  </figure></p>\n<blockquote>\n<p>수많은 트랜스포머 모델이 쏟아져 나오지만 각각 다른 구현 방식으로 인해 활용이 어려웠다. 허깅페이스는 통일된 인터페이스로 이 문제를 해결하고, 복잡한 NLP 파이프라인을 간단하게 구축할 수 있게 해준다.</p>\n</blockquote>\n<h2>0️⃣ 구글 코랩과 깃허브 연동하기</h2>\n<h4>0-1 개발 환경 준비</h4>\n<p>실습을 위해 구글 코랩(Google Colab)과 깃허브(Github) 연동이 필요하다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># GPU 사용 설정 확인</span>\n<span class=\"token keyword\">import</span> torch\ndevice <span class=\"token operator\">=</span> <span class=\"token number\">0</span> <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>is_available<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Using device: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token string\">'GPU'</span> <span class=\"token keyword\">if</span> device <span class=\"token operator\">==</span> <span class=\"token number\">0</span> <span class=\"token keyword\">else</span> <span class=\"token string\">'CPU'</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 필요한 라이브러리 설치</span>\n!pip install transformers datasets torch pandas</code></pre></div>\n<h2>1️⃣ 허깅페이스 개요</h2>\n<h4>1-1 등장 배경</h4>\n<p><strong>💡 문제점</strong></p>\n<ul>\n<li>트랜스포머 아키텍처 기반 모델들이 급속히 증가 ⬆</li>\n<li>모델마다 서로 다른 구현 방식과 인터페이스 존재</li>\n<li>새로운 모델 사용법을 매번 새로 익혀야 하는 어려움</li>\n</ul>\n<p><strong>✅ 해결 방안</strong></p>\n<p>공통된 인터페이스로 다양한 트랜스포머 모델을 활용할 수 있도록 지원하는 라이브러리 개발 → <code class=\"language-text\">Huggingface</code> 팀의 <code class=\"language-text\">transformers</code> 라이브러리</p>\n<h4>1-2 허깅페이스란?</h4>\n<p>다양한 트랜스포머 모델을 <strong>통일된 인터페이스</strong>로 사용 가능하도록 지원하는 오픈소스 라이브러리/플랫폼이다.</p>\n<p><strong>라이브러리 구성</strong></p>\n<ul>\n<li><code class=\"language-text\">transformers</code>: 사전 학습된 트랜스포머 모델 및 토크나이저 지원</li>\n<li><code class=\"language-text\">datasets</code>: 데이터셋 업로드 및 다운로드 지원</li>\n<li>서로 다른 모델을 통일된 인터페이스로 활용 가능</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 통일된 인터페이스 예시</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"What is Huggingface Transformers?\"</span>\n\n<span class=\"token comment\"># BERT 모델 활용</span>\nbert_model <span class=\"token operator\">=</span> AutoModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"bert-base-uncased\"</span><span class=\"token punctuation\">)</span>\nbert_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'bert-base-uncased'</span><span class=\"token punctuation\">)</span>\nencoded_input <span class=\"token operator\">=</span> bert_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\nbert_output <span class=\"token operator\">=</span> bert_model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>encoded_input<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># GPT-2 모델 활용</span>\ngpt_model <span class=\"token operator\">=</span> GPT2LMHeadModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\ngpt_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\nencoded_input <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\ngpt_output <span class=\"token operator\">=</span> gpt_model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>encoded_input<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>플랫폼(Hub)</strong></p>\n<ul>\n<li><code class=\"language-text\">huggingface_hub</code>: 모델과 데이터셋 탐색 및 공유 지원</li>\n<li><a href=\"https://huggingface.co/\">https://huggingface.co/</a></li>\n</ul>\n<h4>1-3 허깅페이스 허브 탐색하기</h4>\n<h5>모델 허브</h5>\n<ul>\n<li><a href=\"https://huggingface.co/models\">https://huggingface.co/models</a></li>\n</ul>\n<p><strong>필터링 옵션</strong></p>\n<table>\n<thead>\n<tr>\n<th>카테고리</th>\n<th>설명</th>\n<th>예시</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Tasks</strong></td>\n<td>작업 종류별 필터링</td>\n<td>NLP, CV, Audio, Multimodal</td>\n</tr>\n<tr>\n<td><strong>Libraries</strong></td>\n<td>학습 라이브러리별 필터링</td>\n<td>PyTorch, TensorFlow, Transformers</td>\n</tr>\n<tr>\n<td><strong>Languages</strong></td>\n<td>지원 언어별 필터링</td>\n<td>한국어, 영어, 중국어</td>\n</tr>\n</tbody>\n</table>\n<p><strong>주요 Task 유형</strong></p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>설명</th>\n<th>모델 예시</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>NLP</strong></td>\n<td>자연어 처리</td>\n<td>번역, 요약, 감정 분석 모델</td>\n</tr>\n<tr>\n<td><strong>CV</strong></td>\n<td>컴퓨터 비전</td>\n<td>이미지 분류, 객체 탐지 모델</td>\n</tr>\n<tr>\n<td><strong>Audio</strong></td>\n<td>오디오 처리</td>\n<td>음성 인식, TTS</td>\n</tr>\n<tr>\n<td><strong>Multimodal</strong></td>\n<td>복합 모달리티</td>\n<td>이미지 캡셔닝, VQA 모델</td>\n</tr>\n</tbody>\n</table>\n<h5>데이터셋 허브</h5>\n<p><a href=\"https://huggingface.co/datasets\">https://huggingface.co/datasets</a></p>\n<p><strong>KLUE Dataset 예시</strong></p>\n<ul>\n<li><strong>Subset</strong>: 데이터셋의 하위 데이터셋으로 각각 다른 학습 목적</li>\n<li><strong>Split</strong>: Train/Validation 8:2 비율로 구성</li>\n</ul>\n<blockquote>\n<p><strong>Train</strong>: 모델 학습에 사용되는 데이터셋으로, 모델이 패턴을 학습하는 데 활용된다.</p>\n<p><strong>Validation</strong>: 모델 성능을 검증하는 데이터셋으로, 학습 과정에서 과적합을 방지하고 모델의 일반화 성능을 평가한다.</p>\n</blockquote>\n<h2>2️⃣ 트랜스포머 모델 활용하기</h2>\n<h4>2-1 모델 구조: 바디 + 헤드</h4>\n<table>\n<thead>\n<tr>\n<th align=\"center\">바디</th>\n<th align=\"center\">헤드</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">모델의 중심 엔진으로 추론 모델의 핵심 부분</td>\n<td align=\"center\">특정 태스크를 수행하는 기능별 세분화된 부분</td>\n</tr>\n</tbody>\n</table>\n<p>허깅페이스 라이브러리로 동일한 바디에 서로 다른 헤드를 붙여 다양한 작업을 수행할 수 있다.</p>\n<h4>2-2 모델 불러오기</h4>\n<p><strong>1. 바디만 불러오기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModel\n\nbase_model_id <span class=\"token operator\">=</span> <span class=\"token string\">'bert-base-uncased'</span>\nbase_model <span class=\"token operator\">=</span> AutoModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>base_model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 출력: hidden state(고차원 벡터)</span>\n<span class=\"token comment\"># 목적: 모델을 feature extractor(특징 추출기)로 활용</span></code></pre></div>\n<p><strong>2. 분류 헤드가 포함된 모델 불러오기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'SamLowe/roberta-base-go_emotions'</span>\nclassification_model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 감정 분류가 이미 학습된 모델</span></code></pre></div>\n<p><strong>3. 분류 헤드가 랜덤 초기화된 모델 불러오기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'klue/roberta-base'</span>\nrandom_model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 바디: 사전학습 완료, 헤드: 랜덤 초기화</span>\n<span class=\"token comment\"># 목적: 특정 task에 맞춰 fine-tuning할 때 사용</span></code></pre></div>\n<h4>2-3 토크나이저 활용하기</h4>\n<h3>토크나이저의 역할</h3>\n<p>텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 <strong>토큰 ID</strong>로 변환하는 아키텍처다.</p>\n<ul>\n<li><strong>토큰</strong>: 언어 모델이 텍스트를 이해하고 생성하는 기본 단위</li>\n<li>학습 데이터를 통해 토큰 사전 구축 → 모델마다 토크나이징 방법이 다름</li>\n<li><strong>동일한 모델 ID</strong>로 모델과 토크나이저를 통일해야 함</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoTokenizer\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'klue/roberta-base'</span>\ntokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>토크나이저 출력값</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoTokenizer\n\ntokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"bert-base-uncased\"</span><span class=\"token punctuation\">)</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"I love banana\"</span>\nencoded <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> truncation<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>encoded<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 출력 예시:</span>\n<span class=\"token comment\"># {</span>\n<span class=\"token comment\">#  'input_ids': [101, 1045, 2293, 15212, 102],</span>\n<span class=\"token comment\">#  'token_type_ids': [0, 0, 0, 0, 0],</span>\n<span class=\"token comment\">#  'attention_mask': [1, 1, 1, 1, 1]</span>\n<span class=\"token comment\"># }</span></code></pre></div>\n<p><strong>주요 출력 요소</strong></p>\n<ul>\n<li><code class=\"language-text\">input_ids</code>: 각 토큰의 사전 내 인덱스 번호</li>\n<li><code class=\"language-text\">token_type_ids</code>: 토큰이 속한 문장 ID (첫 번째 문장: 0, 두 번째 문장: 1)</li>\n<li><code class=\"language-text\">attention_mask</code>: padding token 여부 (실제 토큰: 1, padding: 0)</li>\n</ul>\n<h3>주요 메서드</h3>\n<p><strong>토큰 확인하기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token string\">\"토크나이저는 텍스트를 토큰 단위로 나눈다\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 토큰 리스트로 변환</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>convert_ids_to_tokens<span class=\"token punctuation\">(</span>tokenized<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']</span>\n\n<span class=\"token comment\"># 원래 문장으로 디코딩</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>tokenized<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 토크나이저는 텍스트를 토큰 단위로 나눈다</span></code></pre></div>\n<p><strong>배치 디코딩</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 개별 문장들의 배치 처리</span>\nfirst_tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'첫 번째 문장'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'두 번째 문장'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>batch_decode<span class=\"token punctuation\">(</span>first_tokenized<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']</span>\n\n<span class=\"token comment\"># 문장 쌍으로 처리</span>\nsecond_tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token string\">'첫 번째 문장'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'두 번째 문장'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>batch_decode<span class=\"token punctuation\">(</span>second_tokenized<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']</span></code></pre></div>\n<h2>3️⃣ 데이터셋 활용하기</h2>\n<h4>3-1 데이터셋 다운로드</h4>\n<p><strong>허깅페이스 허브에서 다운로드</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n\n<span class=\"token comment\"># MRC(Machine Reading Comprehension) 데이터셋</span>\nklue_mrc_dataset <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">'klue'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'mrc'</span><span class=\"token punctuation\">)</span>\nklue_mrc_dataset_only_train <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">'klue'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'mrc'</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>로컬 데이터 활용</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># JSON 파일 로드</span>\ndataset_json <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"json\"</span><span class=\"token punctuation\">,</span> data_files<span class=\"token operator\">=</span><span class=\"token string\">\"/path/to/data.json\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 수동으로 train/test 분할</span>\ndataset_json_test <span class=\"token operator\">=</span> dataset_json<span class=\"token punctuation\">[</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"test\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p><strong>Python 딕셔너리/DataFrame 활용</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> Dataset\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token comment\"># 딕셔너리에서 생성</span>\nmy_dict <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"a\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span>\ndataset <span class=\"token operator\">=</span> Dataset<span class=\"token punctuation\">.</span>from_dict<span class=\"token punctuation\">(</span>my_dict<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pandas DataFrame에서 생성</span>\ndf <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"a\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\ndataset <span class=\"token operator\">=</span> Dataset<span class=\"token punctuation\">.</span>from_pandas<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>3-2 데이터셋 가공하기</h4>\n<p><strong>KLUE YNAT 데이터셋 예시</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n\nklue_tc_train <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"ynat\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">)</span>\nklue_tc_eval <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"ynat\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"validation\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 데이터셋 구조 확인</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>klue_tc_train<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Dataset({</span>\n<span class=\"token comment\">#     features: ['guid', 'title', 'label', 'url', 'date'],</span>\n<span class=\"token comment\">#     num_rows: 45678</span>\n<span class=\"token comment\"># })</span></code></pre></div>\n<p><strong>불필요한 컬럼 제거</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">klue_tc_train_removed <span class=\"token operator\">=</span> klue_tc_train<span class=\"token punctuation\">.</span>remove_columns<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'guid'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'url'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>컬럼 추가하기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># label을 문자열로 변환하는 함수</span>\nklue_tc_label <span class=\"token operator\">=</span> klue_tc_train_removed<span class=\"token punctuation\">.</span>features<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">make_str_label</span><span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    batch<span class=\"token punctuation\">[</span><span class=\"token string\">'label_str'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> klue_tc_label<span class=\"token punctuation\">.</span>int2str<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> batch\n\n\n<span class=\"token comment\"># map을 사용하여 배치 처리</span>\nklue_tc_train_removed <span class=\"token operator\">=</span> klue_tc_train_removed<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>\n    make_str_label<span class=\"token punctuation\">,</span>\n    batched<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p><strong>💡 map 함수란?</strong></p>\n<p>데이터셋의 모든 요소에 대해 동일한 함수를 적용하는 허깅페이스 datasets의 핵심 메서드다.</p>\n<ul>\n<li><strong>batched=True</strong>: 여러 샘플을 한 번에 처리하여 성능 향상</li>\n<li><strong>batch_size</strong>: 한 번에 처리할 샘플 수 지정 (메모리 효율성)</li>\n<li><strong>num_proc</strong>: 멀티프로세싱을 통한 병렬 처리 지원</li>\n</ul>\n</blockquote>\n<p><strong>Train/Validation/Test 분할</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 학습용 데이터 1만개 추출</span>\ntrain_dataset <span class=\"token operator\">=</span> klue_tc_train<span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">10000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># 테스트용 데이터 1000개 추출</span>\ndataset <span class=\"token operator\">=</span> klue_tc_eval<span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\ntest_dataset <span class=\"token operator\">=</span> dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># 검증용 데이터 1000개 추출</span>\nvalid_dataset <span class=\"token operator\">=</span> dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h2>4️⃣ 모델을 이용하여 추론하기</h2>\n<h4>4-1 BERT vs GPT 모델 비교</h4>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>BERT</th>\n<th>GPT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>아키텍처</strong></td>\n<td>트랜스포머 인코더만 사용</td>\n<td>트랜스포머 디코더만 사용</td>\n</tr>\n<tr>\n<td><strong>방향성</strong></td>\n<td>양방향(Bidirectional)</td>\n<td>단방향(왼쪽→오른쪽)</td>\n</tr>\n<tr>\n<td><strong>사전학습 목표</strong></td>\n<td>MLM + NSP</td>\n<td>표준 언어 모델링</td>\n</tr>\n<tr>\n<td><strong>주요 강점</strong></td>\n<td>텍스트 이해 및 분석</td>\n<td>텍스트 생성</td>\n</tr>\n<tr>\n<td><strong>활용 분야</strong></td>\n<td>분류, 개체명 인식, 질의응답</td>\n<td>텍스트 생성, 대화형 AI</td>\n</tr>\n<tr>\n<td><strong>활용 방식</strong></td>\n<td>Fine-tuning 중심</td>\n<td>Zero-shot, Few-shot 학습</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>💡 인코더와 디코더</strong></p>\n<p><strong>인코더(Encoder)</strong>: 입력 텍스트를 이해하고 문맥 정보를 추출하는 역할</p>\n<ul>\n<li>입력 전체를 한 번에 보고 양방향으로 정보 처리</li>\n<li>텍스트의 의미를 파악하는 데 특화</li>\n</ul>\n<p><strong>디코더(Decoder)</strong>: 순차적으로 토큰을 생성하는 역할</p>\n<ul>\n<li>이전 토큰들만 참조하여 다음 토큰 예측</li>\n<li>자연스러운 텍스트 생성에 특화</li>\n</ul>\n</blockquote>\n<h4>4-2 BERT 모델로 추론하기</h4>\n<h5>파이프라인 활용</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">\"hykiim/roberta-base-klue-ynat-classification\"</span>\nmodel_pipeline <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span><span class=\"token string\">\"text-classification\"</span><span class=\"token punctuation\">,</span> model<span class=\"token operator\">=</span>model_id<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 텍스트 분류 실행</span>\nresults <span class=\"token operator\">=</span> model_pipeline<span class=\"token punctuation\">(</span>test_dataset<span class=\"token punctuation\">[</span><span class=\"token string\">\"title\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>주요 파이프라인 종류</strong></p>\n<table>\n<thead>\n<tr>\n<th>파이프라인</th>\n<th>작업</th>\n<th>예시</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">text-classification</code></td>\n<td>텍스트 분류</td>\n<td>감정 분석, 스팸 감지</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">token-classification</code></td>\n<td>토큰 분류</td>\n<td>개체명 인식, 품사 태깅</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">text-generation</code></td>\n<td>텍스트 생성</td>\n<td>글쓰기, 코딩</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">question-answering</code></td>\n<td>질의응답</td>\n<td>독해, FAQ 챗봇</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">summarization</code></td>\n<td>요약</td>\n<td>뉴스 기사 요약</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">translation</code></td>\n<td>번역</td>\n<td>다국어 번역</td>\n</tr>\n</tbody>\n</table>\n<h5>커스텀 파이프라인 구현</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">import</span> softmax\n<span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification<span class=\"token punctuation\">,</span> AutoTokenizer\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">CustomPipeline</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> model_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 평가 모드</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__call__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> texts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 텍스트 토크나이징</span>\n        tokenized <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tokenizer<span class=\"token punctuation\">(</span>\n            texts<span class=\"token punctuation\">,</span>\n            return_tensors<span class=\"token operator\">=</span><span class=\"token string\">\"pt\"</span><span class=\"token punctuation\">,</span>\n            padding<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n            truncation<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n        <span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 추론 실행</span>\n        <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            outputs <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>tokenized<span class=\"token punctuation\">)</span>\n            logits <span class=\"token operator\">=</span> outputs<span class=\"token punctuation\">.</span>logits\n\n        <span class=\"token comment\"># 확률 계산 및 예측</span>\n        probabilities <span class=\"token operator\">=</span> softmax<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        scores<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>probabilities<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        labels_str <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">.</span>config<span class=\"token punctuation\">.</span>id2label<span class=\"token punctuation\">[</span>label_idx<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> label_idx <span class=\"token keyword\">in</span> labels<span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"label\"</span><span class=\"token punctuation\">:</span> label<span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">:</span> score<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> <span class=\"token keyword\">for</span> label<span class=\"token punctuation\">,</span> score <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>labels_str<span class=\"token punctuation\">,</span> scores<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># 사용 예시</span>\ncustom_pipeline <span class=\"token operator\">=</span> CustomPipeline<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\nresults <span class=\"token operator\">=</span> custom_pipeline<span class=\"token punctuation\">(</span>test_dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'title'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4>4-3 GPT 모델로 문장 이어쓰기</h4>\n<h5>파이프라인 버전</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># 텍스트 생성 파이프라인</span>\ngenerator <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span><span class=\"token string\">'text-generation'</span><span class=\"token punctuation\">,</span> model<span class=\"token operator\">=</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\nprompt <span class=\"token operator\">=</span> <span class=\"token string\">\"Once upon a time, in a land far, far away,\"</span>\n\n<span class=\"token comment\"># 텍스트 생성</span>\nresults <span class=\"token operator\">=</span> generator<span class=\"token punctuation\">(</span>\n    prompt<span class=\"token punctuation\">,</span>\n    max_length<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">,</span>\n    num_return_sequences<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    no_repeat_ngram_size<span class=\"token operator\">=</span><span class=\"token number\">2</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> result <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"결과 </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>result<span class=\"token punctuation\">[</span><span class=\"token string\">'generated_text'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<h5>직접 구현 버전</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> GPT2LMHeadModel<span class=\"token punctuation\">,</span> AutoTokenizer\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># 모델과 토크나이저 로드</span>\ngpt_model <span class=\"token operator\">=</span> GPT2LMHeadModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\ngpt_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pad 토큰 설정</span>\n<span class=\"token keyword\">if</span> gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token <span class=\"token keyword\">is</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">.</span>eos_token\n\n<span class=\"token comment\"># 텍스트 생성</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"What is Huggingface Transformer?\"</span>\nencoded_input <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\n\noutput_sequences <span class=\"token operator\">=</span> gpt_model<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>\n    input_ids<span class=\"token operator\">=</span>encoded_input<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    max_length<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    num_return_sequences<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    pad_token_id<span class=\"token operator\">=</span>gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token_id<span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    top_p<span class=\"token operator\">=</span><span class=\"token number\">0.95</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.7</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 결과 디코딩</span>\ngenerated_text <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>output_sequences<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Generated Text:\"</span><span class=\"token punctuation\">,</span> generated_text<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>5️⃣ 뉴스기사 요약→번역→감정분석 파이프라인</h2>\n<h4>5-1 파이프라인 설계</h4>\n<p><strong>전체 워크플로우</strong></p>\n<ol>\n<li>기사 내용(한국어) → 요약(한국어)</li>\n<li>요약(한국어) → 번역(영어)</li>\n<li>번역(영어) → 감정분석</li>\n</ol>\n<h4>5-2 데이터셋 준비</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token comment\"># KLUE MRC 데이터셋 로드</span>\nfull_dataset <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"mrc\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 실습용 데이터 일부 선택</span>\nnum_samples_to_use <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nklue_mrc_subset <span class=\"token operator\">=</span> full_dataset<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_samples_to_use<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"로드된 데이터셋 정보:\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>klue_mrc_subset<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>5-3 1단계: 기사 요약</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 요약 모델 파이프라인 로드</span>\nsummarizer <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"summarization\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"gogamza/kobart-summarization\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 요약 함수 정의</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">summarize_context</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    summary_result <span class=\"token operator\">=</span> summarizer<span class=\"token punctuation\">(</span>\n        example<span class=\"token punctuation\">[</span><span class=\"token string\">'context'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        max_length<span class=\"token operator\">=</span><span class=\"token number\">150</span><span class=\"token punctuation\">,</span>\n        min_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>\n        do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span>\n    <span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> summary_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'summary_text'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># 데이터셋에 요약 적용</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"요약 작업을 시작합니다...\"</span><span class=\"token punctuation\">)</span>\nsummarized_dataset <span class=\"token operator\">=</span> klue_mrc_subset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>summarize_context<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"요약 작업 완료.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4>5-4 2단계: 한영 번역</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 번역 모델 파이프라인 로드</span>\ntranslator <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"translation\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"facebook/nllb-200-distilled-600M\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 번역 함수 정의</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">translate_summary_to_english</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    translation_result <span class=\"token operator\">=</span> translator<span class=\"token punctuation\">(</span>\n        example<span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        src_lang<span class=\"token operator\">=</span><span class=\"token string\">\"kor_Hang\"</span><span class=\"token punctuation\">,</span>\n        tgt_lang<span class=\"token operator\">=</span><span class=\"token string\">\"eng_Latn\"</span><span class=\"token punctuation\">,</span>\n        max_length<span class=\"token operator\">=</span><span class=\"token number\">150</span><span class=\"token punctuation\">,</span>\n        min_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>\n        do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span>\n    <span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> translation_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'translation_text'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># 데이터셋에 번역 적용</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"번역 작업을 시작합니다...\"</span><span class=\"token punctuation\">)</span>\ntranslated_dataset <span class=\"token operator\">=</span> summarized_dataset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>translate_summary_to_english<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"번역 작업 완료.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4>5-5 3단계: 감정 분석</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 감정 분석 모델 파이프라인 로드</span>\nemotion_classifier <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-classification\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"SamLowe/roberta-base-go_emotions\"</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 감정 분석 함수 정의</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">analyze_emotion</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    emotion_result <span class=\"token operator\">=</span> emotion_classifier<span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'emotion'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> emotion_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># 데이터셋에 감정 분석 적용</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"감정 분석 작업을 시작합니다...\"</span><span class=\"token punctuation\">)</span>\nfinal_dataset <span class=\"token operator\">=</span> translated_dataset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>analyze_emotion<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"감정 분석 작업 완료.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4>5-6 결과 확인</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"최종 데이터셋 컬럼:\"</span><span class=\"token punctuation\">,</span> final_dataset<span class=\"token punctuation\">.</span>column_names<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 최종 결과 확인</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>final_dataset<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"\\n--- 샘플 </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> ---\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"원문 일부: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'context'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">100]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">...\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"요약: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"영어 번역: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"감정 분석: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'emotion'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># CSV로 저장 (선택사항)</span>\ndf_final <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>final_dataset<span class=\"token punctuation\">)</span>\ndf_final<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'news_analysis_pipeline_results.csv'</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>✅ 정리</h2>\n<p><strong>⭐️ 허깅페이스의 핵심 가치</strong></p>\n<ul>\n<li>🔄 <strong>통일된 인터페이스</strong>: 다양한 모델을 동일한 방식으로 활용</li>\n<li>🚀 <strong>간편한 파이프라인</strong>: 복잡한 전처리 과정을 자동화</li>\n<li>🛠️ <strong>유연한 커스터마이징</strong>: 필요에 따라 세부 구현 가능</li>\n<li>🌐 <strong>풍부한 생태계</strong>: 모델부터 데이터셋까지 원스톱 지원</li>\n</ul>\n<p><strong>👀 파이프라인 구축을 어떻게??</strong></p>\n<ul>\n<li><strong>모델 선택</strong>: 도메인과 언어에 적합한 모델 활용</li>\n<li><strong>배치 처리</strong>: <code class=\"language-text\">map</code> 함수를 활용한 효율적인 데이터 처리</li>\n<li><strong>에러 핸들링</strong>: 각 단계별 예외 처리 및 로깅</li>\n<li><strong>성능 최적화</strong>: GPU 활용과 적절한 batch_size 설정</li>\n</ul>\n<p>허깅페이스는 복잡한 NLP 파이프라인을 누구나 쉽게 구축할 수 있게 해주는 강력한 도구다. 기본 사용법부터 복합 파이프라인까지 단계적으로 학습하여 실무에서 효과적으로 활용할 수 있다.</p>","frontmatter":{"title":"허깅페이스로 파이프라인을 만들어보자 - 트랜스포머 모델 활용 가이드","date":"October 04, 2025","description":"허깅페이스 라이브러리로 NLP 모델을 쉽게 활용하는 방법부터 복합 파이프라인 구축까지 단계별로 알아보자.","pointColor":"#FF9A00"}},"previous":{"fields":{"slug":"/RAG/"},"frontmatter":{"title":"RAG의 핵심 개념과 실습"}},"next":null},"pageContext":{"id":"4f2ebd57-1b6b-5267-9012-58a0c9529d93","previousPostId":"816bf1bd-f98e-5ef5-95ed-ddf55630cee3","nextPostId":null}},"staticQueryHashes":["2841359383"],"slicesMap":{}}