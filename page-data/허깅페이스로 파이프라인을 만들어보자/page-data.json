{"componentChunkName":"component---src-templates-blog-post-js","path":"/í—ˆê¹…í˜ì´ìŠ¤ë¡œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ë³´ì/","result":{"data":{"site":{"siteMetadata":{"title":"ë©”ì¸"}},"markdownRemark":{"id":"4f2ebd57-1b6b-5267-9012-58a0c9529d93","excerpt":"ìˆ˜ë§ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ìŸì•„ì ¸ ë‚˜ì˜¤ì§€ë§Œ ê°ê° ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ìœ¼ë¡œ ì¸í•´ í™œìš©ì´ ì–´ë ¤ì› ë‹¤. í—ˆê¹…í˜ì´ìŠ¤ëŠ” í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë³µì¡í•œ NLP íŒŒì´í”„ë¼ì¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. 0ï¸âƒ£ êµ¬ê¸€ ì½”ë©ê³¼ ê¹ƒí—ˆë¸Œ ì—°ë™í•˜ê¸° 0-â€¦","html":"<p><figure class=\"gatsby-resp-image-figure\" style=\"background: transparent;\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.400000000000006%; position: relative; bottom: 0; left: 0; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/1b3d79daf52aff8152fd2b54c697b855/bc904/index.webp 250w,\n/static/1b3d79daf52aff8152fd2b54c697b855/4be29/index.webp 500w,\n/static/1b3d79daf52aff8152fd2b54c697b855/f8b1b/index.webp 600w\"\n              sizes=\"(max-width: 600px) 100vw, 600px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/1b3d79daf52aff8152fd2b54c697b855/43fa5/index.png 250w,\n/static/1b3d79daf52aff8152fd2b54c697b855/c6e3d/index.png 500w,\n/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png 600w\"\n            sizes=\"(max-width: 600px) 100vw, 600px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/1b3d79daf52aff8152fd2b54c697b855/ff59c/index.png\"\n            alt=\"ã„±ã…‡ã…‡\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">ã„±ã…‡ã…‡</figcaption>\n  </figure></p>\n<blockquote>\n<p>ìˆ˜ë§ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ìŸì•„ì ¸ ë‚˜ì˜¤ì§€ë§Œ ê°ê° ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ìœ¼ë¡œ ì¸í•´ í™œìš©ì´ ì–´ë ¤ì› ë‹¤. í—ˆê¹…í˜ì´ìŠ¤ëŠ” í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë³µì¡í•œ NLP íŒŒì´í”„ë¼ì¸ì„ ê°„ë‹¨í•˜ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.</p>\n</blockquote>\n<h2 id=\"0ï¸-êµ¬ê¸€-ì½”ë©ê³¼-ê¹ƒí—ˆë¸Œ-ì—°ë™í•˜ê¸°\">0ï¸âƒ£ êµ¬ê¸€ ì½”ë©ê³¼ ê¹ƒí—ˆë¸Œ ì—°ë™í•˜ê¸°</h2>\n<h4 id=\"0-1-ê°œë°œ-í™˜ê²½-ì¤€ë¹„\">0-1 ê°œë°œ í™˜ê²½ ì¤€ë¹„</h4>\n<p>ì‹¤ìŠµì„ ìœ„í•´ êµ¬ê¸€ ì½”ë©(Google Colab)ê³¼ ê¹ƒí—ˆë¸Œ(Github) ì—°ë™ì´ í•„ìš”í•˜ë‹¤.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># GPU ì‚¬ìš© ì„¤ì • í™•ì¸</span>\n<span class=\"token keyword\">import</span> torch\ndevice <span class=\"token operator\">=</span> <span class=\"token number\">0</span> <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>is_available<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Using device: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token string\">'GPU'</span> <span class=\"token keyword\">if</span> device <span class=\"token operator\">==</span> <span class=\"token number\">0</span> <span class=\"token keyword\">else</span> <span class=\"token string\">'CPU'</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜</span>\n!pip install transformers datasets torch pandas</code></pre></div>\n<h2 id=\"1ï¸-í—ˆê¹…í˜ì´ìŠ¤-ê°œìš”\">1ï¸âƒ£ í—ˆê¹…í˜ì´ìŠ¤ ê°œìš”</h2>\n<h4 id=\"1-1-ë“±ì¥-ë°°ê²½\">1-1 ë“±ì¥ ë°°ê²½</h4>\n<p><strong>ğŸ’¡ ë¬¸ì œì </strong></p>\n<ul>\n<li>íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ë“¤ì´ ê¸‰ì†íˆ ì¦ê°€ â¬†</li>\n<li>ëª¨ë¸ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ê³¼ ì¸í„°í˜ì´ìŠ¤ ì¡´ì¬</li>\n<li>ìƒˆë¡œìš´ ëª¨ë¸ ì‚¬ìš©ë²•ì„ ë§¤ë²ˆ ìƒˆë¡œ ìµí˜€ì•¼ í•˜ëŠ” ì–´ë ¤ì›€</li>\n</ul>\n<p><strong>âœ… í•´ê²° ë°©ì•ˆ</strong></p>\n<p>ê³µí†µëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œë°œ â†’ <code class=\"language-text\">Huggingface</code> íŒ€ì˜ <code class=\"language-text\">transformers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬</p>\n<h4 id=\"1-2-í—ˆê¹…í˜ì´ìŠ¤ë€\">1-2 í—ˆê¹…í˜ì´ìŠ¤ë€?</h4>\n<p>ë‹¤ì–‘í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ <strong>í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤</strong>ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì§€ì›í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬/í”Œë«í¼ì´ë‹¤.</p>\n<p><strong>ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì„±</strong></p>\n<ul>\n<li><code class=\"language-text\">transformers</code>: ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì§€ì›</li>\n<li><code class=\"language-text\">datasets</code>: ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë° ë‹¤ìš´ë¡œë“œ ì§€ì›</li>\n<li>ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ í™œìš© ê°€ëŠ¥</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ ì˜ˆì‹œ</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"What is Huggingface Transformers?\"</span>\n\n<span class=\"token comment\"># BERT ëª¨ë¸ í™œìš©</span>\nbert_model <span class=\"token operator\">=</span> AutoModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"bert-base-uncased\"</span><span class=\"token punctuation\">)</span>\nbert_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'bert-base-uncased'</span><span class=\"token punctuation\">)</span>\nencoded_input <span class=\"token operator\">=</span> bert_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\nbert_output <span class=\"token operator\">=</span> bert_model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>encoded_input<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># GPT-2 ëª¨ë¸ í™œìš©</span>\ngpt_model <span class=\"token operator\">=</span> GPT2LMHeadModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\ngpt_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\nencoded_input <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\ngpt_output <span class=\"token operator\">=</span> gpt_model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>encoded_input<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>í”Œë«í¼(Hub)</strong></p>\n<ul>\n<li><code class=\"language-text\">huggingface_hub</code>: ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ íƒìƒ‰ ë° ê³µìœ  ì§€ì›</li>\n<li><a href=\"https://huggingface.co/\">https://huggingface.co/</a></li>\n</ul>\n<h4 id=\"1-3-í—ˆê¹…í˜ì´ìŠ¤-í—ˆë¸Œ-íƒìƒ‰í•˜ê¸°\">1-3 í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œ íƒìƒ‰í•˜ê¸°</h4>\n<h5>ëª¨ë¸ í—ˆë¸Œ</h5>\n<ul>\n<li><a href=\"https://huggingface.co/models\">https://huggingface.co/models</a></li>\n</ul>\n<p><strong>í•„í„°ë§ ì˜µì…˜</strong></p>\n<table>\n<thead>\n<tr>\n<th>ì¹´í…Œê³ ë¦¬</th>\n<th>ì„¤ëª…</th>\n<th>ì˜ˆì‹œ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Tasks</strong></td>\n<td>ì‘ì—… ì¢…ë¥˜ë³„ í•„í„°ë§</td>\n<td>NLP, CV, Audio, Multimodal</td>\n</tr>\n<tr>\n<td><strong>Libraries</strong></td>\n<td>í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ í•„í„°ë§</td>\n<td>PyTorch, TensorFlow, Transformers</td>\n</tr>\n<tr>\n<td><strong>Languages</strong></td>\n<td>ì§€ì› ì–¸ì–´ë³„ í•„í„°ë§</td>\n<td>í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´</td>\n</tr>\n</tbody>\n</table>\n<p><strong>ì£¼ìš” Task ìœ í˜•</strong></p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>ì„¤ëª…</th>\n<th>ëª¨ë¸ ì˜ˆì‹œ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>NLP</strong></td>\n<td>ìì—°ì–´ ì²˜ë¦¬</td>\n<td>ë²ˆì—­, ìš”ì•½, ê°ì • ë¶„ì„ ëª¨ë¸</td>\n</tr>\n<tr>\n<td><strong>CV</strong></td>\n<td>ì»´í“¨í„° ë¹„ì „</td>\n<td>ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€ ëª¨ë¸</td>\n</tr>\n<tr>\n<td><strong>Audio</strong></td>\n<td>ì˜¤ë””ì˜¤ ì²˜ë¦¬</td>\n<td>ìŒì„± ì¸ì‹, TTS</td>\n</tr>\n<tr>\n<td><strong>Multimodal</strong></td>\n<td>ë³µí•© ëª¨ë‹¬ë¦¬í‹°</td>\n<td>ì´ë¯¸ì§€ ìº¡ì…”ë‹, VQA ëª¨ë¸</td>\n</tr>\n</tbody>\n</table>\n<h5>ë°ì´í„°ì…‹ í—ˆë¸Œ</h5>\n<p><a href=\"https://huggingface.co/datasets\">https://huggingface.co/datasets</a></p>\n<p><strong>KLUE Dataset ì˜ˆì‹œ</strong></p>\n<ul>\n<li><strong>Subset</strong>: ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°ê° ë‹¤ë¥¸ í•™ìŠµ ëª©ì </li>\n<li><strong>Split</strong>: Train/Validation 8:2 ë¹„ìœ¨ë¡œ êµ¬ì„±</li>\n</ul>\n<blockquote>\n<p><strong>Train</strong>: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° í™œìš©ëœë‹¤.</p>\n<p><strong>Validation</strong>: ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, í•™ìŠµ ê³¼ì •ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.</p>\n</blockquote>\n<h2 id=\"2ï¸-íŠ¸ëœìŠ¤í¬ë¨¸-ëª¨ë¸-í™œìš©í•˜ê¸°\">2ï¸âƒ£ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í™œìš©í•˜ê¸°</h2>\n<h4 id=\"2-1-ëª¨ë¸-êµ¬ì¡°-ë°”ë””--í—¤ë“œ\">2-1 ëª¨ë¸ êµ¬ì¡°: ë°”ë”” + í—¤ë“œ</h4>\n<table>\n<thead>\n<tr>\n<th align=\"center\">ë°”ë””</th>\n<th align=\"center\">í—¤ë“œ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">ëª¨ë¸ì˜ ì¤‘ì‹¬ ì—”ì§„ìœ¼ë¡œ ì¶”ë¡  ëª¨ë¸ì˜ í•µì‹¬ ë¶€ë¶„</td>\n<td align=\"center\">íŠ¹ì • íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê¸°ëŠ¥ë³„ ì„¸ë¶„í™”ëœ ë¶€ë¶„</td>\n</tr>\n</tbody>\n</table>\n<p>í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë™ì¼í•œ ë°”ë””ì— ì„œë¡œ ë‹¤ë¥¸ í—¤ë“œë¥¼ ë¶™ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.</p>\n<h4 id=\"2-2-ëª¨ë¸-ë¶ˆëŸ¬ì˜¤ê¸°\">2-2 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°</h4>\n<p><strong>1. ë°”ë””ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModel\n\nbase_model_id <span class=\"token operator\">=</span> <span class=\"token string\">'bert-base-uncased'</span>\nbase_model <span class=\"token operator\">=</span> AutoModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>base_model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ì¶œë ¥: hidden state(ê³ ì°¨ì› ë²¡í„°)</span>\n<span class=\"token comment\"># ëª©ì : ëª¨ë¸ì„ feature extractor(íŠ¹ì§• ì¶”ì¶œê¸°)ë¡œ í™œìš©</span></code></pre></div>\n<p><strong>2. ë¶„ë¥˜ í—¤ë“œê°€ í¬í•¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'SamLowe/roberta-base-go_emotions'</span>\nclassification_model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ê°ì • ë¶„ë¥˜ê°€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸</span></code></pre></div>\n<p><strong>3. ë¶„ë¥˜ í—¤ë“œê°€ ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'klue/roberta-base'</span>\nrandom_model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ë°”ë””: ì‚¬ì „í•™ìŠµ ì™„ë£Œ, í—¤ë“œ: ëœë¤ ì´ˆê¸°í™”</span>\n<span class=\"token comment\"># ëª©ì : íŠ¹ì • taskì— ë§ì¶° fine-tuningí•  ë•Œ ì‚¬ìš©</span></code></pre></div>\n<h4 id=\"2-3-í† í¬ë‚˜ì´ì €-í™œìš©í•˜ê¸°\">2-3 í† í¬ë‚˜ì´ì € í™œìš©í•˜ê¸°</h4>\n<p><strong>í† í¬ë‚˜ì´ì €ì˜ ì—­í• </strong></p>\n<p>í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê° í† í°ì„ ëŒ€ì‘í•˜ëŠ” <strong>í† í° ID</strong>ë¡œ ë³€í™˜í•˜ëŠ” ì•„í‚¤í…ì²˜ë‹¤.</p>\n<ul>\n<li><strong>í† í°</strong>: ì–¸ì–´ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„</li>\n<li>í•™ìŠµ ë°ì´í„°ë¥¼ í†µí•´ í† í° ì‚¬ì „ êµ¬ì¶• â†’ ëª¨ë¸ë§ˆë‹¤ í† í¬ë‚˜ì´ì§• ë°©ë²•ì´ ë‹¤ë¦„</li>\n<li><strong>ë™ì¼í•œ ëª¨ë¸ ID</strong>ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í†µì¼í•´ì•¼ í•¨</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoTokenizer\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">'klue/roberta-base'</span>\ntokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>í† í¬ë‚˜ì´ì € ì¶œë ¥ê°’</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoTokenizer\n\ntokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"bert-base-uncased\"</span><span class=\"token punctuation\">)</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"I love banana\"</span>\nencoded <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> truncation<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>encoded<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ì¶œë ¥ ì˜ˆì‹œ:</span>\n<span class=\"token comment\"># {</span>\n<span class=\"token comment\">#  'input_ids': [101, 1045, 2293, 15212, 102],</span>\n<span class=\"token comment\">#  'token_type_ids': [0, 0, 0, 0, 0],</span>\n<span class=\"token comment\">#  'attention_mask': [1, 1, 1, 1, 1]</span>\n<span class=\"token comment\"># }</span></code></pre></div>\n<p><strong>ì£¼ìš” ì¶œë ¥ ìš”ì†Œ</strong></p>\n<ul>\n<li><code class=\"language-text\">input_ids</code>: ê° í† í°ì˜ ì‚¬ì „ ë‚´ ì¸ë±ìŠ¤ ë²ˆí˜¸</li>\n<li><code class=\"language-text\">token_type_ids</code>: í† í°ì´ ì†í•œ ë¬¸ì¥ ID (ì²« ë²ˆì§¸ ë¬¸ì¥: 0, ë‘ ë²ˆì§¸ ë¬¸ì¥: 1)</li>\n<li><code class=\"language-text\">attention_mask</code>: padding token ì—¬ë¶€ (ì‹¤ì œ í† í°: 1, padding: 0)</li>\n</ul>\n<p><strong>í† í° í™•ì¸í•˜ê¸°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token string\">\"í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>convert_ids_to_tokens<span class=\"token punctuation\">(</span>tokenized<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS]', 'í† í¬', '##ë‚˜ì´', '##ì €', '##ëŠ”', 'í…ìŠ¤íŠ¸', '##ë¥¼', 'í† ', '##í°', 'ë‹¨ìœ„', '##ë¡œ', 'ë‚˜ëˆˆë‹¤', '[SEP]']</span>\n\n<span class=\"token comment\"># ì›ë˜ ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>tokenized<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤</span></code></pre></div>\n<p><strong>ë°°ì¹˜ ë””ì½”ë”©</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ê°œë³„ ë¬¸ì¥ë“¤ì˜ ë°°ì¹˜ ì²˜ë¦¬</span>\nfirst_tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'ì²« ë²ˆì§¸ ë¬¸ì¥'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'ë‘ ë²ˆì§¸ ë¬¸ì¥'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>batch_decode<span class=\"token punctuation\">(</span>first_tokenized<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS] ì²« ë²ˆì§¸ ë¬¸ì¥ [SEP]', '[CLS] ë‘ ë²ˆì§¸ ë¬¸ì¥ [SEP]']</span>\n\n<span class=\"token comment\"># ë¬¸ì¥ ìŒìœ¼ë¡œ ì²˜ë¦¬</span>\nsecond_tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token string\">'ì²« ë²ˆì§¸ ë¬¸ì¥'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'ë‘ ë²ˆì§¸ ë¬¸ì¥'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>batch_decode<span class=\"token punctuation\">(</span>second_tokenized<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['[CLS] ì²« ë²ˆì§¸ ë¬¸ì¥ [SEP] ë‘ ë²ˆì§¸ ë¬¸ì¥ [SEP]']</span></code></pre></div>\n<h2 id=\"3ï¸-ë°ì´í„°ì…‹-í™œìš©í•˜ê¸°\">3ï¸âƒ£ ë°ì´í„°ì…‹ í™œìš©í•˜ê¸°</h2>\n<h4 id=\"3-1-ë°ì´í„°ì…‹-ë‹¤ìš´ë¡œë“œ\">3-1 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ</h4>\n<p><strong>í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n\n<span class=\"token comment\"># MRC(Machine Reading Comprehension) ë°ì´í„°ì…‹</span>\nklue_mrc_dataset <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">'klue'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'mrc'</span><span class=\"token punctuation\">)</span>\nklue_mrc_dataset_only_train <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">'klue'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'mrc'</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>ë¡œì»¬ ë°ì´í„° í™œìš©</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># JSON íŒŒì¼ ë¡œë“œ</span>\ndataset_json <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"json\"</span><span class=\"token punctuation\">,</span> data_files<span class=\"token operator\">=</span><span class=\"token string\">\"/path/to/data.json\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ìˆ˜ë™ìœ¼ë¡œ train/test ë¶„í• </span>\ndataset_json_test <span class=\"token operator\">=</span> dataset_json<span class=\"token punctuation\">[</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"test\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p><strong>Python ë”•ì…”ë„ˆë¦¬/DataFrame í™œìš©</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> Dataset\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token comment\"># ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±</span>\nmy_dict <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"a\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span>\ndataset <span class=\"token operator\">=</span> Dataset<span class=\"token punctuation\">.</span>from_dict<span class=\"token punctuation\">(</span>my_dict<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pandas DataFrameì—ì„œ ìƒì„±</span>\ndf <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"a\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\ndataset <span class=\"token operator\">=</span> Dataset<span class=\"token punctuation\">.</span>from_pandas<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"3-2-ë°ì´í„°ì…‹-ê°€ê³µí•˜ê¸°\">3-2 ë°ì´í„°ì…‹ ê°€ê³µí•˜ê¸°</h4>\n<p><strong>KLUE YNAT ë°ì´í„°ì…‹ ì˜ˆì‹œ</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n\nklue_tc_train <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"ynat\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">)</span>\nklue_tc_eval <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"ynat\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"validation\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>klue_tc_train<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Dataset({</span>\n<span class=\"token comment\">#     features: ['guid', 'title', 'label', 'url', 'date'],</span>\n<span class=\"token comment\">#     num_rows: 45678</span>\n<span class=\"token comment\"># })</span></code></pre></div>\n<p><strong>ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">klue_tc_train_removed <span class=\"token operator\">=</span> klue_tc_train<span class=\"token punctuation\">.</span>remove_columns<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'guid'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'url'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>ì»¬ëŸ¼ ì¶”ê°€í•˜ê¸°</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># labelì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜</span>\nklue_tc_label <span class=\"token operator\">=</span> klue_tc_train_removed<span class=\"token punctuation\">.</span>features<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">make_str_label</span><span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    batch<span class=\"token punctuation\">[</span><span class=\"token string\">'label_str'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> klue_tc_label<span class=\"token punctuation\">.</span>int2str<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> batch\n\n\n<span class=\"token comment\"># mapì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬</span>\nklue_tc_train_removed <span class=\"token operator\">=</span> klue_tc_train_removed<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>\n    make_str_label<span class=\"token punctuation\">,</span>\n    batched<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p><strong>ğŸ’¡ map í•¨ìˆ˜ë€?</strong></p>\n<p>ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìš”ì†Œì— ëŒ€í•´ ë™ì¼í•œ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” í—ˆê¹…í˜ì´ìŠ¤ datasetsì˜ í•µì‹¬ ë©”ì„œë“œë‹¤.</p>\n<ul>\n<li><strong>batched=True</strong>: ì—¬ëŸ¬ ìƒ˜í”Œì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ</li>\n<li><strong>batch_size</strong>: í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ ì§€ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)</li>\n<li><strong>num_proc</strong>: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›</li>\n</ul>\n</blockquote>\n<p><strong>Train/Validation/Test ë¶„í• </strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># í•™ìŠµìš© ë°ì´í„° 1ë§Œê°œ ì¶”ì¶œ</span>\ntrain_dataset <span class=\"token operator\">=</span> klue_tc_train<span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">10000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° 1000ê°œ ì¶”ì¶œ</span>\ndataset <span class=\"token operator\">=</span> klue_tc_eval<span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\ntest_dataset <span class=\"token operator\">=</span> dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># ê²€ì¦ìš© ë°ì´í„° 1000ê°œ ì¶”ì¶œ</span>\nvalid_dataset <span class=\"token operator\">=</span> dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>train_test_split<span class=\"token punctuation\">(</span>test_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h2 id=\"4ï¸-ëª¨ë¸ì„-ì´ìš©í•˜ì—¬-ì¶”ë¡ í•˜ê¸°\">4ï¸âƒ£ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì¶”ë¡ í•˜ê¸°</h2>\n<h4 id=\"4-1-bert-vs-gpt-ëª¨ë¸-ë¹„êµ\">4-1 BERT vs GPT ëª¨ë¸ ë¹„êµ</h4>\n<table>\n<thead>\n<tr>\n<th>íŠ¹ì§•</th>\n<th>BERT</th>\n<th>GPT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>ì•„í‚¤í…ì²˜</strong></td>\n<td>íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë§Œ ì‚¬ìš©</td>\n<td>íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ë§Œ ì‚¬ìš©</td>\n</tr>\n<tr>\n<td><strong>ë°©í–¥ì„±</strong></td>\n<td>ì–‘ë°©í–¥(Bidirectional)</td>\n<td>ë‹¨ë°©í–¥(ì™¼ìª½â†’ì˜¤ë¥¸ìª½)</td>\n</tr>\n<tr>\n<td><strong>ì‚¬ì „í•™ìŠµ ëª©í‘œ</strong></td>\n<td>MLM + NSP</td>\n<td>í‘œì¤€ ì–¸ì–´ ëª¨ë¸ë§</td>\n</tr>\n<tr>\n<td><strong>ì£¼ìš” ê°•ì </strong></td>\n<td>í…ìŠ¤íŠ¸ ì´í•´ ë° ë¶„ì„</td>\n<td>í…ìŠ¤íŠ¸ ìƒì„±</td>\n</tr>\n<tr>\n<td><strong>í™œìš© ë¶„ì•¼</strong></td>\n<td>ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ì§ˆì˜ì‘ë‹µ</td>\n<td>í…ìŠ¤íŠ¸ ìƒì„±, ëŒ€í™”í˜• AI</td>\n</tr>\n<tr>\n<td><strong>í™œìš© ë°©ì‹</strong></td>\n<td>Fine-tuning ì¤‘ì‹¬</td>\n<td>Zero-shot, Few-shot í•™ìŠµ</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>ğŸ’¡ ì¸ì½”ë”ì™€ ë””ì½”ë”</strong></p>\n<p><strong>ì¸ì½”ë”(Encoder)</strong>: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ê³  ë¬¸ë§¥ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì—­í• </p>\n<ul>\n<li>ì…ë ¥ ì „ì²´ë¥¼ í•œ ë²ˆì— ë³´ê³  ì–‘ë°©í–¥ìœ¼ë¡œ ì •ë³´ ì²˜ë¦¬</li>\n<li>í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ë° íŠ¹í™”</li>\n</ul>\n<p><strong>ë””ì½”ë”(Decoder)</strong>: ìˆœì°¨ì ìœ¼ë¡œ í† í°ì„ ìƒì„±í•˜ëŠ” ì—­í• </p>\n<ul>\n<li>ì´ì „ í† í°ë“¤ë§Œ ì°¸ì¡°í•˜ì—¬ ë‹¤ìŒ í† í° ì˜ˆì¸¡</li>\n<li>ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„±ì— íŠ¹í™”</li>\n</ul>\n</blockquote>\n<h4 id=\"4-2-bert-ëª¨ë¸ë¡œ-ì¶”ë¡ í•˜ê¸°\">4-2 BERT ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ê¸°</h4>\n<h5>íŒŒì´í”„ë¼ì¸ í™œìš©</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n\nmodel_id <span class=\"token operator\">=</span> <span class=\"token string\">\"hykiim/roberta-base-klue-ynat-classification\"</span>\nmodel_pipeline <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span><span class=\"token string\">\"text-classification\"</span><span class=\"token punctuation\">,</span> model<span class=\"token operator\">=</span>model_id<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤í–‰</span>\nresults <span class=\"token operator\">=</span> model_pipeline<span class=\"token punctuation\">(</span>test_dataset<span class=\"token punctuation\">[</span><span class=\"token string\">\"title\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>ì£¼ìš” íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜</strong></p>\n<table>\n<thead>\n<tr>\n<th>íŒŒì´í”„ë¼ì¸</th>\n<th>ì‘ì—…</th>\n<th>ì˜ˆì‹œ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">text-classification</code></td>\n<td>í…ìŠ¤íŠ¸ ë¶„ë¥˜</td>\n<td>ê°ì • ë¶„ì„, ìŠ¤íŒ¸ ê°ì§€</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">token-classification</code></td>\n<td>í† í° ë¶„ë¥˜</td>\n<td>ê°œì²´ëª… ì¸ì‹, í’ˆì‚¬ íƒœê¹…</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">text-generation</code></td>\n<td>í…ìŠ¤íŠ¸ ìƒì„±</td>\n<td>ê¸€ì“°ê¸°, ì½”ë”©</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">question-answering</code></td>\n<td>ì§ˆì˜ì‘ë‹µ</td>\n<td>ë…í•´, FAQ ì±—ë´‡</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">summarization</code></td>\n<td>ìš”ì•½</td>\n<td>ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">translation</code></td>\n<td>ë²ˆì—­</td>\n<td>ë‹¤êµ­ì–´ ë²ˆì—­</td>\n</tr>\n</tbody>\n</table>\n<h5>ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ êµ¬í˜„</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">import</span> softmax\n<span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> AutoModelForSequenceClassification<span class=\"token punctuation\">,</span> AutoTokenizer\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">CustomPipeline</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> model_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>model <span class=\"token operator\">=</span> AutoModelForSequenceClassification<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># í‰ê°€ ëª¨ë“œ</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__call__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> texts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•</span>\n        tokenized <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tokenizer<span class=\"token punctuation\">(</span>\n            texts<span class=\"token punctuation\">,</span>\n            return_tensors<span class=\"token operator\">=</span><span class=\"token string\">\"pt\"</span><span class=\"token punctuation\">,</span>\n            padding<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n            truncation<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n        <span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># ì¶”ë¡  ì‹¤í–‰</span>\n        <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            outputs <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>tokenized<span class=\"token punctuation\">)</span>\n            logits <span class=\"token operator\">=</span> outputs<span class=\"token punctuation\">.</span>logits\n\n        <span class=\"token comment\"># í™•ë¥  ê³„ì‚° ë° ì˜ˆì¸¡</span>\n        probabilities <span class=\"token operator\">=</span> softmax<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        scores<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>probabilities<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        labels_str <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">.</span>config<span class=\"token punctuation\">.</span>id2label<span class=\"token punctuation\">[</span>label_idx<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> label_idx <span class=\"token keyword\">in</span> labels<span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"label\"</span><span class=\"token punctuation\">:</span> label<span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">:</span> score<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> <span class=\"token keyword\">for</span> label<span class=\"token punctuation\">,</span> score <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>labels_str<span class=\"token punctuation\">,</span> scores<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># ì‚¬ìš© ì˜ˆì‹œ</span>\ncustom_pipeline <span class=\"token operator\">=</span> CustomPipeline<span class=\"token punctuation\">(</span>model_id<span class=\"token punctuation\">)</span>\nresults <span class=\"token operator\">=</span> custom_pipeline<span class=\"token punctuation\">(</span>test_dataset<span class=\"token punctuation\">[</span><span class=\"token string\">'title'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"4-3-gpt-ëª¨ë¸ë¡œ-ë¬¸ì¥-ì´ì–´ì“°ê¸°\">4-3 GPT ëª¨ë¸ë¡œ ë¬¸ì¥ ì´ì–´ì“°ê¸°</h4>\n<h5>íŒŒì´í”„ë¼ì¸ ë²„ì „</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸</span>\ngenerator <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span><span class=\"token string\">'text-generation'</span><span class=\"token punctuation\">,</span> model<span class=\"token operator\">=</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\nprompt <span class=\"token operator\">=</span> <span class=\"token string\">\"Once upon a time, in a land far, far away,\"</span>\n\n<span class=\"token comment\"># í…ìŠ¤íŠ¸ ìƒì„±</span>\nresults <span class=\"token operator\">=</span> generator<span class=\"token punctuation\">(</span>\n    prompt<span class=\"token punctuation\">,</span>\n    max_length<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">,</span>\n    num_return_sequences<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    no_repeat_ngram_size<span class=\"token operator\">=</span><span class=\"token number\">2</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> result <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"ê²°ê³¼ </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>result<span class=\"token punctuation\">[</span><span class=\"token string\">'generated_text'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<h5>ì§ì ‘ êµ¬í˜„ ë²„ì „</h5>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> GPT2LMHeadModel<span class=\"token punctuation\">,</span> AutoTokenizer\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ</span>\ngpt_model <span class=\"token operator\">=</span> GPT2LMHeadModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\ngpt_tokenizer <span class=\"token operator\">=</span> AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">'gpt2'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pad í† í° ì„¤ì •</span>\n<span class=\"token keyword\">if</span> gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token <span class=\"token keyword\">is</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">.</span>eos_token\n\n<span class=\"token comment\"># í…ìŠ¤íŠ¸ ìƒì„±</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"What is Huggingface Transformer?\"</span>\nencoded_input <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">'pt'</span><span class=\"token punctuation\">)</span>\n\noutput_sequences <span class=\"token operator\">=</span> gpt_model<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>\n    input_ids<span class=\"token operator\">=</span>encoded_input<span class=\"token punctuation\">[</span><span class=\"token string\">'input_ids'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    max_length<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    num_return_sequences<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    pad_token_id<span class=\"token operator\">=</span>gpt_tokenizer<span class=\"token punctuation\">.</span>pad_token_id<span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    top_p<span class=\"token operator\">=</span><span class=\"token number\">0.95</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.7</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ê²°ê³¼ ë””ì½”ë”©</span>\ngenerated_text <span class=\"token operator\">=</span> gpt_tokenizer<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>output_sequences<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Generated Text:\"</span><span class=\"token punctuation\">,</span> generated_text<span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"5ï¸-ë‰´ìŠ¤ê¸°ì‚¬-ìš”ì•½ë²ˆì—­ê°ì •ë¶„ì„-íŒŒì´í”„ë¼ì¸\">5ï¸âƒ£ ë‰´ìŠ¤ê¸°ì‚¬ ìš”ì•½â†’ë²ˆì—­â†’ê°ì •ë¶„ì„ íŒŒì´í”„ë¼ì¸</h2>\n<h4 id=\"5-1-íŒŒì´í”„ë¼ì¸-ì„¤ê³„\">5-1 íŒŒì´í”„ë¼ì¸ ì„¤ê³„</h4>\n<p><strong>ì „ì²´ ì›Œí¬í”Œë¡œìš°</strong></p>\n<ol>\n<li>ê¸°ì‚¬ ë‚´ìš©(í•œêµ­ì–´) â†’ ìš”ì•½(í•œêµ­ì–´)</li>\n<li>ìš”ì•½(í•œêµ­ì–´) â†’ ë²ˆì—­(ì˜ì–´)</li>\n<li>ë²ˆì—­(ì˜ì–´) â†’ ê°ì •ë¶„ì„</li>\n</ol>\n<h4 id=\"5-2-ë°ì´í„°ì…‹-ì¤€ë¹„\">5-2 ë°ì´í„°ì…‹ ì¤€ë¹„</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> datasets <span class=\"token keyword\">import</span> load_dataset\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token comment\"># KLUE MRC ë°ì´í„°ì…‹ ë¡œë“œ</span>\nfull_dataset <span class=\"token operator\">=</span> load_dataset<span class=\"token punctuation\">(</span><span class=\"token string\">\"klue\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"mrc\"</span><span class=\"token punctuation\">,</span> split<span class=\"token operator\">=</span><span class=\"token string\">\"train\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ì‹¤ìŠµìš© ë°ì´í„° ì¼ë¶€ ì„ íƒ</span>\nnum_samples_to_use <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nklue_mrc_subset <span class=\"token operator\">=</span> full_dataset<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_samples_to_use<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ë¡œë“œëœ ë°ì´í„°ì…‹ ì •ë³´:\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>klue_mrc_subset<span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"5-3-1ë‹¨ê³„-ê¸°ì‚¬-ìš”ì•½\">5-3 1ë‹¨ê³„: ê¸°ì‚¬ ìš”ì•½</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ìš”ì•½ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ</span>\nsummarizer <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"summarization\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"gogamza/kobart-summarization\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ìš”ì•½ í•¨ìˆ˜ ì •ì˜</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">summarize_context</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    summary_result <span class=\"token operator\">=</span> summarizer<span class=\"token punctuation\">(</span>\n        example<span class=\"token punctuation\">[</span><span class=\"token string\">'context'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        max_length<span class=\"token operator\">=</span><span class=\"token number\">150</span><span class=\"token punctuation\">,</span>\n        min_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>\n        do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span>\n    <span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> summary_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'summary_text'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># ë°ì´í„°ì…‹ì— ìš”ì•½ ì ìš©</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ìš”ì•½ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\"</span><span class=\"token punctuation\">)</span>\nsummarized_dataset <span class=\"token operator\">=</span> klue_mrc_subset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>summarize_context<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ìš”ì•½ ì‘ì—… ì™„ë£Œ.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"5-4-2ë‹¨ê³„-í•œì˜-ë²ˆì—­\">5-4 2ë‹¨ê³„: í•œì˜ ë²ˆì—­</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ë²ˆì—­ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ</span>\ntranslator <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"translation\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"facebook/nllb-200-distilled-600M\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ë²ˆì—­ í•¨ìˆ˜ ì •ì˜</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">translate_summary_to_english</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    translation_result <span class=\"token operator\">=</span> translator<span class=\"token punctuation\">(</span>\n        example<span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        src_lang<span class=\"token operator\">=</span><span class=\"token string\">\"kor_Hang\"</span><span class=\"token punctuation\">,</span>\n        tgt_lang<span class=\"token operator\">=</span><span class=\"token string\">\"eng_Latn\"</span><span class=\"token punctuation\">,</span>\n        max_length<span class=\"token operator\">=</span><span class=\"token number\">150</span><span class=\"token punctuation\">,</span>\n        min_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>\n        do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span>\n    <span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> translation_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'translation_text'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># ë°ì´í„°ì…‹ì— ë²ˆì—­ ì ìš©</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ë²ˆì—­ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\"</span><span class=\"token punctuation\">)</span>\ntranslated_dataset <span class=\"token operator\">=</span> summarized_dataset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>translate_summary_to_english<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ë²ˆì—­ ì‘ì—… ì™„ë£Œ.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"5-5-3ë‹¨ê³„-ê°ì •-ë¶„ì„\">5-5 3ë‹¨ê³„: ê°ì • ë¶„ì„</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ê°ì • ë¶„ì„ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ</span>\nemotion_classifier <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-classification\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"SamLowe/roberta-base-go_emotions\"</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>device\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ê°ì • ë¶„ì„ í•¨ìˆ˜ ì •ì˜</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">analyze_emotion</span><span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    emotion_result <span class=\"token operator\">=</span> emotion_classifier<span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    example<span class=\"token punctuation\">[</span><span class=\"token string\">'emotion'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> emotion_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> example\n\n<span class=\"token comment\"># ë°ì´í„°ì…‹ì— ê°ì • ë¶„ì„ ì ìš©</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ê°ì • ë¶„ì„ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\"</span><span class=\"token punctuation\">)</span>\nfinal_dataset <span class=\"token operator\">=</span> translated_dataset<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>analyze_emotion<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ê°ì • ë¶„ì„ ì‘ì—… ì™„ë£Œ.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"5-6-ê²°ê³¼-í™•ì¸\">5-6 ê²°ê³¼ í™•ì¸</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ìµœì¢… ë°ì´í„°ì…‹ ì»¬ëŸ¼:\"</span><span class=\"token punctuation\">,</span> final_dataset<span class=\"token punctuation\">.</span>column_names<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ìµœì¢… ê²°ê³¼ í™•ì¸</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>final_dataset<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"\\n--- ìƒ˜í”Œ </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> ---\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"ì›ë¬¸ ì¼ë¶€: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'context'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">100]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">...\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"ìš”ì•½: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"ì˜ì–´ ë²ˆì—­: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'english_summary'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"ê°ì • ë¶„ì„: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>final_dataset<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'emotion'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># CSVë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)</span>\ndf_final <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>final_dataset<span class=\"token punctuation\">)</span>\ndf_final<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'news_analysis_pipeline_results.csv'</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"-ì •ë¦¬\">âœ… ì •ë¦¬</h2>\n<p><strong>â­ï¸ í—ˆê¹…í˜ì´ìŠ¤ì˜ í•µì‹¬ ê°€ì¹˜</strong></p>\n<ul>\n<li>ğŸ”„ <strong>í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤</strong>: ë‹¤ì–‘í•œ ëª¨ë¸ì„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í™œìš©</li>\n<li>ğŸš€ <strong>ê°„í¸í•œ íŒŒì´í”„ë¼ì¸</strong>: ë³µì¡í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìë™í™”</li>\n<li>ğŸ› ï¸ <strong>ìœ ì—°í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•</strong>: í•„ìš”ì— ë”°ë¼ ì„¸ë¶€ êµ¬í˜„ ê°€ëŠ¥</li>\n<li>ğŸŒ <strong>í’ë¶€í•œ ìƒíƒœê³„</strong>: ëª¨ë¸ë¶€í„° ë°ì´í„°ì…‹ê¹Œì§€ ì›ìŠ¤í†± ì§€ì›</li>\n</ul>\n<p><strong>ğŸ‘€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì„ ì–´ë–»ê²Œ??</strong></p>\n<ul>\n<li><strong>ëª¨ë¸ ì„ íƒ</strong>: ë„ë©”ì¸ê³¼ ì–¸ì–´ì— ì í•©í•œ ëª¨ë¸ í™œìš©</li>\n<li><strong>ë°°ì¹˜ ì²˜ë¦¬</strong>: <code class=\"language-text\">map</code> í•¨ìˆ˜ë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬</li>\n<li><strong>ì—ëŸ¬ í•¸ë“¤ë§</strong>: ê° ë‹¨ê³„ë³„ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…</li>\n<li><strong>ì„±ëŠ¥ ìµœì í™”</strong>: GPU í™œìš©ê³¼ ì ì ˆí•œ batch_size ì„¤ì •</li>\n</ul>\n<p>í—ˆê¹…í˜ì´ìŠ¤ëŠ” ë³µì¡í•œ NLP íŒŒì´í”„ë¼ì¸ì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. ê¸°ë³¸ ì‚¬ìš©ë²•ë¶€í„° ë³µí•© íŒŒì´í”„ë¼ì¸ê¹Œì§€ ë‹¨ê³„ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì‹¤ë¬´ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.</p>","frontmatter":{"title":"í—ˆê¹…í˜ì´ìŠ¤ë¡œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ë³´ì - íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í™œìš© ê°€ì´ë“œ","date":"October 04, 2025","description":"í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ NLP ëª¨ë¸ì„ ì‰½ê²Œ í™œìš©í•˜ëŠ” ë°©ë²•ë¶€í„° ë³µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë³´ì.","pointColor":"#FF9A00"}},"previous":{"fields":{"slug":"/RAG/"},"frontmatter":{"title":"RAGì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ìŠµ"}},"next":{"fields":{"slug":"/ë°ì´í„°ë² ì´ìŠ¤ - Chapter2/"},"frontmatter":{"title":"ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ê°œë¡  ğŸ“š - Chapter2. ERDë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„í•˜ê¸°"}}},"pageContext":{"id":"4f2ebd57-1b6b-5267-9012-58a0c9529d93","previousPostId":"816bf1bd-f98e-5ef5-95ed-ddf55630cee3","nextPostId":"b9f4b15e-3c04-52fb-8f49-9e9e3bb46490"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}